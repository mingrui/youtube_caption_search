okay so last class of part one
00:00:00.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m00s

I guess the theme of part one is
00:00:05.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m05s

classification and regression with deep
00:00:09.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m09s

wading and specifically it's about
00:00:12.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m12s

identifying and learning the best
00:00:15.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m15s

practices for classification and
00:00:17.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m17s

regression and we started out with the
00:00:19.439
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m19s

kind of here are three lines of code to
00:00:23.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m23s

do image classification and gradually
00:00:25.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m25s

we've been for the first four lessons
00:00:28.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m28s

within kind of going through NLP
00:00:31.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m31s

structured data collaborative filtering
00:00:34.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m34s

and kind of understanding some of the
00:00:36.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m36s

key pieces and most importantly
00:00:38.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m38s

understanding you know how to actually
00:00:39.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m39s

make these things work well in practice
00:00:42.629
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m42s

and then the last three lessons are then
00:00:44.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m44s

kind of going back over all of those
00:00:47.879
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m47s

topics in kind of reverse order to
00:00:49.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m49s

understand more detail about what was
00:00:52.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m52s

going on and understanding what the code
00:00:53.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m53s

looks like behind the scenes and wanting
00:00:56.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m56s

to kind of write them from scratch part
00:00:57.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h00m57s

two of the course we'll move from a
00:01:02.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m02s

focus on classification and regression
00:01:07.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m07s

which is kind of predicting a thing like
00:01:09.119
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m09s

a number or or at most a small number of
00:01:12.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m12s

things like a small number of labels and
00:01:15.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m15s

we'll focus more on generative modeling
00:01:17.479
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m17s

generative modeling means predicting
00:01:20.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m20s

kind of lots of things
00:01:23.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m23s

for example creating a sentence such as
00:01:25.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m25s

in Ural translation or image captioning
00:01:28.439
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m28s

or question-answering while creating an
00:01:31.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m31s

image such as in style transfer
00:01:33.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m33s

super-resolution segmentation and so
00:01:38.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m38s

forth and then in part two it'll move
00:01:41.939
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m41s

away from being just here are some best
00:01:46.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m46s

practices you know established best
00:01:49.649
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m49s

practices either through people that are
00:01:52.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m52s

written papers or through research that
00:01:54.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m54s

last day is done and it kind of got
00:01:56.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m56s

convinced that these are best practices
00:01:58.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m58s

to some stuff which would be a little
00:01:59.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h01m59s

bit more speculative you know some stuff
00:02:02.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m02s

which is maybe recent papers that
00:02:04.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m04s

haven't been fully tested yet and
00:02:07.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m07s

sometimes in part two like pickles will
00:02:11.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m11s

come out in the middle of the course and
00:02:12.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m12s

will change direction with the course
00:02:14.519
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m14s

and study that paper because it's just
00:02:16.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m16s

you know interesting and so if you're
00:02:18.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m18s

interested in kind of learning a bit
00:02:20.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m20s

more about how to read a paper and how
00:02:23.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m23s

to implement it from scratch and so
00:02:26.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m26s

forth then that's another good reason to
00:02:28.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m28s

do part two it still doesn't assume any
00:02:30.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m30s

particular math background but it does
00:02:34.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m34s

beyond kind of high school but but it
00:02:37.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m37s

does assume that you're prepared to
00:02:39.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m39s

spend time like you know digging through
00:02:41.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m41s

the notation and understanding it and
00:02:44.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m44s

converting it to code and so forth all
00:02:46.859
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m46s

right so we're we're up to is is our
00:02:50.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m50s

intent at the moment and I think one of
00:02:52.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m52s

the issues I find most with teaching
00:02:55.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m55s

iron ends is trying to ensure that
00:02:58.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h02m58s

people understand they're not in any way
00:03:00.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m00s

different or unusual or magical they're
00:03:03.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m03s

they're just a standard fully connected
00:03:05.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m05s

Network and so let's go back to the
00:03:07.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m07s

standard fully connected Network which
00:03:11.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m11s

looks like this right so to remind you
00:03:13.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m13s

the arrows represent one or more layer
00:03:15.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m15s

operations generally speaking a linear
00:03:19.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m19s

followed by a nonlinear function in this
00:03:22.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m22s

case their matrix modifications followed
00:03:26.069
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m26s

by real new raw or fan and the arrows of
00:03:29.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m29s

the same color represent the same
00:03:34.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m34s

exactly the same weight matrix being
00:03:37.129
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m37s

used and so one thing which was just
00:03:39.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m39s

slightly different from previous fully
00:03:43.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m43s

connected networks we've seen is that we
00:03:45.359
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m45s

have an input coming in at the not just
00:03:47.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m47s

at the first layer but also for the
00:03:51.959
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m51s

second layer and also at the third layer
00:03:53.069
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m53s

and we tried a couple of approaches one
00:03:54.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m54s

was concatenating the inputs and one was
00:03:56.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m56s

adding the airport's okay but there was
00:03:58.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h03m58s

nothing at all conceptually different
00:04:00.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m00s

about this so that code looked like this
00:04:03.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m03s

we had a model where we basically
00:04:09.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m09s

defined the the three arrows colors we
00:04:15.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m15s

had as three different weight matrices
00:04:18.299
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m18s

okay and by using the linear
00:04:19.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m19s

we got actually both the weight matrix
00:04:25.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m25s

and the bias vector wrapped up for free
00:04:26.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m26s

for us and then we went through and we
00:04:29.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m29s

did each of our embeddings put it
00:04:33.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m33s

through our first linear layer and then
00:04:35.879
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m35s

we did each of our we call them hiddens
00:04:39.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m39s

being the orange orange areas and in
00:04:42.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m42s

order to avoid the fact that there's no
00:04:48.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m48s

orange arrow coming into the first one
00:04:51.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m51s

we decided to kind of invent an empty
00:04:54.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m54s

matrix and that way every one of these
00:04:57.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m57s

rows about the same right and so then we
00:04:59.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h04m59s

did exactly the same thing except we
00:05:02.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m02s

used to loop just to refactor the cutter
00:05:08.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m08s

okay so it's just it was just a code
00:05:11.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m11s

refactoring there was no change of
00:05:13.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m13s

anything conceptually and since we were
00:05:16.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m16s

doing a refactoring we took advantage of
00:05:19.289
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m19s

that to increase the number of
00:05:21.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m21s

characters to eight because I was too
00:05:23.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m23s

lazy to type 8 when the alias but I'm
00:05:25.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m25s

quite happy to change the loop in that
00:05:27.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m27s

stage
00:05:30.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m30s

yeah so this now looked through this
00:05:30.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m30s

exact same thing but we had eight of
00:05:34.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m34s

these rather than three so then we
00:05:36.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m36s

refactored that again by taking
00:05:43.919
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m43s

advantage of an end errand in which
00:05:46.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m46s

basically puts that loop together for us
00:05:49.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m49s

and keeps track of the this.h as it goes
00:05:51.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m51s

along for us and so by using that we
00:05:57.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h05m57s

were able to replace the loop with a
00:06:01.229
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m01s

single call and so again that's just a
00:06:04.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m04s

refactoring doing exactly the same thing
00:06:07.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m07s

okay so then we looked at something
00:06:14.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m14s

which was mainly designed to save some
00:06:17.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m17s

training time which was previously we
00:06:21.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m21s

had if we had a big piece of text right
00:06:27.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m27s

so we've got like a movie review
00:06:33.419
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m33s

but we were basically splitting it up
00:06:36.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m36s

into eight character segments and we'd
00:06:39.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m39s

grab like segment number one and use
00:06:43.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m43s

that to predict the next character right
00:06:46.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m46s

but in order to make sure that we kind
00:06:50.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m50s

of used all of the data we didn't just
00:06:52.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m52s

put it up like that we actually said
00:06:54.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m54s

like okay here's our whole thing let's
00:06:57.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h06m57s

grab the first will be to grab this
00:07:01.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m01s

section the second will be to grab that
00:07:03.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m03s

section in that section then that
00:07:06.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m06s

section and each time would predict
00:07:08.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m08s

predicting the next one character a lot
00:07:10.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m10s

okay and so you know I was bit concerned
00:07:13.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m13s

that that seems pretty wasteful because
00:07:17.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m17s

like as we calculate this section nearly
00:07:19.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m19s

all of it overlaps with the previous
00:07:22.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m22s

section okay so instead what we did was
00:07:24.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m24s

we said all right well what if we
00:07:28.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m28s

actually did split it into
00:07:30.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m30s

non-overlapping pieces right and we said
00:07:32.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m32s

all right let's grab this section here
00:07:37.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m37s

and use it to predict every one of the
00:07:41.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m41s

characters one along right and then
00:07:46.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m46s

let's grab this section here and use it
00:07:49.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m49s

to predict every one of the characters
00:07:51.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m51s

went along so after we look at the first
00:07:53.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m53s

character in we try to predict the
00:07:55.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m55s

second character and then now if we look
00:07:57.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m57s

at the second character we try to
00:07:59.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h07m59s

predict the third character and so okay
00:08:01.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m01s

and so that's where you've got to and
00:08:03.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m03s

then what if you perceptive folks asked
00:08:05.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m05s

a really interesting question or
00:08:08.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m08s

expressed their concern which was hey
00:08:10.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m10s

after we got through the first the first
00:08:13.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m13s

point here after we got through the
00:08:16.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m16s

first point here we kind of withdrew
00:08:22.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m22s

away our H activations and started a new
00:08:25.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m25s

one which meant that when it was trying
00:08:30.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m30s

to use character one to predict
00:08:32.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m32s

character - it's got nothing to go on
00:08:36.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m36s

you know it hasn't built it's only built
00:08:39.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m39s

it's only done one linear layer and so
00:08:41.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m41s

that seems like a problem which indeed
00:08:44.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m44s

it is okay
00:08:47.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m47s

so we're going to do the obvious thing
00:08:49.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m49s

which is let's not throw away H okay so
00:08:50.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m50s

let's not throw away that that matrix at
00:08:54.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m54s

all
00:08:58.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m58s

so in code the big problem is here but
00:08:59.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h08m59s

every time we call forward so in other
00:09:04.589
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m04s

words every time we do a new mini-batch
00:09:06.899
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m06s

we're creating our our hidden state
00:09:08.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m08s

right which remember is the orange
00:09:12.589
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m12s

circles okay we're resetting it back to
00:09:16.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m16s

a bunch of zeros and so as we go to the
00:09:18.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m18s

next non-overlapping section we're
00:09:21.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m21s

saying forget everything that's come
00:09:23.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m23s

before but in fact the whole point is we
00:09:25.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m25s

know exactly where we are we're at the
00:09:28.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m28s

end of the previous section and about to
00:09:30.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m30s

start the new next contiguous section so
00:09:31.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m31s

let's not throw it away
00:09:33.839
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m33s

so instead the idea would be to cut this
00:09:34.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m34s

out right move it up to here okay store
00:09:37.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m37s

it away in self and then kind of keep
00:09:45.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m45s

updating it right now so we're going to
00:09:49.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m49s

do that and there's going to be some
00:09:51.959
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m51s

minor details to get right so let's
00:09:54.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m54s

start by looking at the model so here's
00:09:58.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h09m58s

the model it's it's nearly identical and
00:10:02.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m02s

okay
00:10:09.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m09s

here's the model it's nearly identical
00:10:10.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m10s

but I've got as expected one more line
00:10:12.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m12s

in my constructor where I call something
00:10:15.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m15s

called init hidden and as expected in it
00:10:17.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m17s

hidden sets self dot H to be a bunch of
00:10:20.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m20s

zeros okay so that's entirely
00:10:26.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m26s

unsurprising and then as you can see our
00:10:30.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m30s

R and n now takes in self garage and it
00:10:33.839
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m33s

as before spits out our new hidden
00:10:39.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m39s

activations and so now the trick is to
00:10:42.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m42s

now store that away inside self dot H
00:10:45.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m45s

and so here's wrinkle number one if you
00:10:49.709
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m49s

think about it if I was to simply do it
00:10:53.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m53s

like like that right and now I train
00:10:56.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h10m56s

this
00:11:02.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m02s

on a document that's I don't know a
00:11:02.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m02s

million words million characters long
00:11:04.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m04s

then the size of this unrolled are a 10
00:11:08.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m08s

is has a million circles here and so
00:11:12.149
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m12s

that's fine going forwards right there
00:11:18.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m18s

when I finally get to the end and I say
00:11:21.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m21s

here's my character and actually
00:11:22.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m22s

remember we're doing multi output now so
00:11:24.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m24s

multi output looks like this right or if
00:11:26.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m26s

we were to draw the unrolled version of
00:11:30.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m30s

multi output we would have a triangle
00:11:32.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m32s

coming off at every point okay so the
00:11:34.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m34s

problem is that then when we do back
00:11:39.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m39s

propagation we're calculating you know
00:11:42.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m42s

how much does the error at character one
00:11:44.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m44s

impact the final answer how much does
00:11:49.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m49s

the error character to impact the final
00:11:52.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m52s

answer and so forth and so we need to go
00:11:54.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m54s

back through and say like how do we have
00:11:56.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h11m56s

to update our weights based on all of
00:12:00.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m00s

those you know errors and so if there
00:12:02.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m02s

are our million characters my unrolled R
00:12:05.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m05s

and N is a million layers long I have a
00:12:09.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m09s

1 million layer fully connected Network
00:12:12.209
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m12s

all right and like I didn't have to
00:12:15.529
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m15s

write the million layers because I have
00:12:18.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m18s

for loop and the for loops hidden away
00:12:19.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m19s

behind that you know the self dot R and
00:12:21.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m21s

n but it's still there right we so so
00:12:24.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m24s

this is actually a 1 million layer fully
00:12:28.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m28s

connected Network and so the problem
00:12:31.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m31s

with that is it's going to be very
00:12:33.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m33s

memory intensive because in order to do
00:12:34.829
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m34s

the chain rule I have to be able to
00:12:37.439
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m37s

multiply at every step like you know
00:12:39.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m39s

after a few times she acts right and so
00:12:42.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m42s

like I've got that means I have to
00:12:46.829
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m46s

remember that those values you the value
00:12:48.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m48s

of every set of layers so I'm gonna have
00:12:50.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m50s

to remember all those million layers and
00:12:53.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m53s

I'm going to do have to have to do a
00:12:54.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m54s

million multiplications and I'm going to
00:12:56.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m56s

have to do that every batch okay so that
00:12:57.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h12m57s

would be bad so to avoid that we
00:13:01.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m01s

basically say all right well from time
00:13:05.399
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m05s

to time I want you to forget your
00:13:07.649
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m07s

history okay so we can still remember
00:13:10.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m10s

the state right which is to remember
00:13:13.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m13s

like what's the
00:13:15.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m15s

values in our hidden matrix right but we
00:13:16.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m16s

can remember the state without
00:13:19.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m19s

remembering everything about how we got
00:13:21.569
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m21s

there
00:13:23.519
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m23s

so there's a little function called
00:13:23.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m23s

repackage variable which literally is
00:13:27.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m27s

just this right it just simply says grab
00:13:35.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m35s

the tensor out of it
00:13:41.189
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m41s

right because remember the tensor itself
00:13:42.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m42s

doesn't have any concept of history
00:13:44.939
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m44s

right and create a new variable out of
00:13:46.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m46s

that and so this variables going to have
00:13:49.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m49s

the same value but no no history of
00:13:51.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m51s

operations and therefore when it tries
00:13:55.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m55s

to back propagate it all it'll stop
00:13:57.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m57s

there
00:13:59.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h13m59s

so basically what we're going to do then
00:14:00.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m00s

is we're going to call this in our
00:14:02.579
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m02s

forward so that means it's going to do
00:14:05.279
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m05s

add characters it's going to back
00:14:07.529
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m07s

propagate through eight layers it's
00:14:09.899
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m09s

going to keep track of the actual values
00:14:13.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m13s

in our hidden state but it's going to
00:14:15.779
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m15s

throw away at the end of those eight
00:14:18.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m18s

it's its history of operations so this
00:14:20.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m20s

is this approach it's called back prop
00:14:24.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m24s

through time and you know when you read
00:14:27.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m27s

about it online people make it sound
00:14:30.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m30s

like like a different algorithm or some
00:14:32.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m32s

big insight or something but it's it's
00:14:36.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m36s

not at all right it's just saying hey
00:14:38.399
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m38s

after our for loop you know just throw
00:14:40.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m40s

away your your history operations and
00:14:43.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m43s

start afresh so we're keeping our hidden
00:14:46.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m46s

state but we're not keeping our hidden
00:14:48.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m48s

States history okay so that's that's
00:14:50.879
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m50s

wrinkle number one that's what this
00:14:56.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m56s

repackage bar is doing and so what do
00:14:58.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h14m58s

you see
00:15:00.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m00s

BP BP TT that's referring to that crop
00:15:01.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m01s

through time and you might remember we
00:15:05.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m05s

saw that in our original errand in
00:15:07.439
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m07s

Lesson we had a variable called BP t t
00:15:10.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m10s

equals 70 and so when we set that
00:15:14.129
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m14s

they're actually saying how many layers
00:15:16.589
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m16s

backprop through another good reason not
00:15:18.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m18s

to back crop through too many layers is
00:15:22.019
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m22s

if you have any kind of gradient
00:15:23.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m23s

instability like gradient explosion or
00:15:26.149
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m26s

gradients
00:15:28.649
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m28s

banishing you know too many more of the
00:15:29.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m29s

more layers you have the harder the
00:15:31.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m31s

network s to Train so smaller and less
00:15:33.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m33s

resilient on the other hand and longer
00:15:37.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m37s

value for VP TT means that you're able
00:15:40.569
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m40s

to explicitly capture a longer kind of
00:15:43.479
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m43s

memory more state okay so that's a
00:15:48.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m48s

that's something that you get to tune
00:15:53.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m53s

when you create your area
00:15:55.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h15m55s

all right wrinkle number two is how are
00:16:00.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m00s

we going to put the data into this right
00:16:06.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m06s

like it's all very well the way I
00:16:09.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m09s

described it just now where we said you
00:16:12.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m12s

know we could do this and we can first
00:16:19.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m19s

of all look at this section then this
00:16:24.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m24s

section in this section but we're going
00:16:27.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m27s

to do a mini batch at a time right we
00:16:30.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m30s

want to do a bunch at a time
00:16:33.069
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m33s

so in other words we want to say let's
00:16:35.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m35s

do it like this
00:16:43.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m43s

so mini-batch number one would say let's
00:16:50.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m50s

look at this section and predict that
00:16:53.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m53s

section and at the same time in parallel
00:16:56.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m56s

let's look at this totally different
00:16:59.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h16m59s

section and predict this and at the same
00:17:01.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m01s

time in parallel let's look at this
00:17:04.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m04s

totally different section and predict
00:17:06.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m06s

this right and so then because remember
00:17:08.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m08s

in our in our hidden state we have a
00:17:12.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m12s

vector of hidden state for everything in
00:17:16.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m16s

our mini batch right so it's going to
00:17:18.589
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m18s

keep track of at the end of this is
00:17:20.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m20s

going to be a you know a vector here a
00:17:22.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m22s

vector here a vector here and then we
00:17:23.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m23s

can move across to the next one and say
00:17:25.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m25s

okay so this part of the mini batch use
00:17:28.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m28s

this to predict that and use this to
00:17:31.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m31s

predict that and use this to predict
00:17:34.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m34s

that right so you can see that we're
00:17:36.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m36s

moving that we've got like a number of
00:17:38.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m38s

totally separate bits of our text that
00:17:40.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m40s

we're moving through in parallel right
00:17:43.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m43s

so hopefully this is going to ring a few
00:17:46.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m46s

bells for you because what happened was
00:17:48.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m48s

was back when we started looking at
00:17:53.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m53s

torch texture the first time we started
00:17:56.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m56s

talking about how it creates these mini
00:17:58.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h17m58s

batches and I said what happened was we
00:18:00.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m00s

took our whole big long document
00:18:03.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m03s

consisting of like you know the entire
00:18:07.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m07s

works of nature or all of the IMDB
00:18:10.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m10s

reviews concatenated together or
00:18:13.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m13s

whatever and a lot of a lot of you not
00:18:15.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m15s

surprisingly because this really said
00:18:17.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m17s

this is really weird at first a lot of
00:18:19.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m19s

you didn't quite hear what I said
00:18:20.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m20s

correctly what I said was we split this
00:18:22.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m22s

into 64 equal sized chunks and a lot of
00:18:24.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m24s

your brains when Jeremy just said we
00:18:29.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m29s

split this into chunks of size 64 but
00:18:31.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m31s

that's not what Theresa Jeremy said we
00:18:35.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m35s

split it into 64 equal sized chunks
00:18:37.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m37s

right so if this whole thing was length
00:18:40.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m40s

64 million right which would be a
00:18:42.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m42s

reasonable sized corpus not an unusual
00:18:45.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m45s

size corpus then each of our 64 chunks
00:18:49.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m49s

would have been of length 1 million
00:18:51.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m51s

right and so then what we did was we
00:18:55.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m55s

talked the first chunk of 1 million and
00:18:58.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h18m58s

we put it here
00:19:01.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m01s

and then we took the second chunk of 1
00:19:03.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m03s

million and we put it here
00:19:04.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m04s

the third chunk of 1 million we put it
00:19:06.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m06s

here and so forth to create 64 chunks
00:19:08.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m08s

and then H mini-batch consisted of us
00:19:13.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m13s

going let's split this down here and
00:19:18.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m18s

here and here and each of these is of
00:19:22.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m22s

size BP te T which I think we had
00:19:27.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m27s

something like 70 right and so what
00:19:34.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m34s

happened was we said alright let's look
00:19:37.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m37s

at our first mini batch is all of these
00:19:39.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m39s

right so we do all of those at once and
00:19:42.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m42s

predict everything accrue off set by one
00:19:45.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m45s

and then at the end of that first mini
00:19:49.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m49s

batch we went to the second chunk right
00:19:53.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m53s

and used each one of these to predict
00:19:57.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m57s

the next one offset by one ok so that's
00:19:58.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h19m58s

that's why we did that slightly weird
00:20:03.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m03s

thing right is that we wanted to have a
00:20:06.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m06s

bunch of things we can look through in
00:20:08.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m08s

parallel each of which like hopefully a
00:20:10.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m10s

far enough away from each other you know
00:20:13.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m13s

that we don't have to worry about the
00:20:15.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m15s

fact that you know the truth is this
00:20:17.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m17s

starting the start of this million
00:20:19.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m19s

characters was actually in the middle of
00:20:21.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m21s

a sentence but you know who cares right
00:20:23.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m23s

because it's you know that only happens
00:20:26.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m26s

once every million characters honey I
00:20:28.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m28s

was wondering if you could talk a little
00:20:33.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m33s

bit more about augmentation for this
00:20:36.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m36s

kind of data set and how to data
00:20:38.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m38s

augmentation of this kind of data said
00:20:41.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m41s

yeah no I can't because I don't I really
00:20:42.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m42s

know a good way it's one of the things
00:20:46.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m46s

I'm going to be studying between now and
00:20:49.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m49s

part two there have been some recent
00:20:52.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m52s

developments particularly something we
00:20:55.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m55s

talked about the machine learning course
00:20:58.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h20m58s

and I think we've refinished in here
00:21:00.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m00s

which was somebody for a recent careful
00:21:01.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m01s

competition won it by doing data
00:21:04.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m04s

augmentation by randomly inserting parts
00:21:06.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m06s

of different rows basic
00:21:12.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m12s

something like that may be useful here
00:21:16.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m16s

and I've seen it I've seen some papers
00:21:18.289
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m18s

that do something like that but yeah I
00:21:20.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m20s

haven't seen any kind of recent ish
00:21:23.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m23s

state-of-the-art new
00:21:26.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m26s

NLP papers that that are doing this kind
00:21:28.129
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m28s

of data orientation so it's something
00:21:32.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m32s

we're planning to work on so it's
00:21:33.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m33s

generally how the issues be PTT so
00:21:39.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m39s

there's a couple of things to think
00:21:47.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m47s

about when you pick your be PTT the
00:21:48.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m48s

first is that you'll note that the the
00:21:49.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m49s

matrix size for a mini batch has a B PTT
00:21:52.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m52s

the the TT by batch size so one issue is
00:21:57.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h21m57s

your GPU Ram needs to be able to fit
00:22:08.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m08s

that by your embedding matrix racks
00:22:11.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m11s

every one of these is going to have B of
00:22:14.899
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m14s

length embedding length plus all of the
00:22:17.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m17s

hidden state so one thing is to you know
00:22:20.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m20s

if you get a cruder out of memory error
00:22:23.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m23s

you need to reduce one of those if
00:22:25.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m25s

you're finding your training is very
00:22:29.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m29s

unstable like your loss is shooting off
00:22:33.139
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m33s

to LAN suddenly then you could try to
00:22:35.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m35s

decreasing your B PTT because you've got
00:22:38.629
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m38s

less layers to gradient explode through
00:22:40.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m40s

it's too slow you could try decreasing
00:22:43.779
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m43s

your B PTT because it's going to kind of
00:22:47.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m47s

do one of those steps at a time like
00:22:49.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m49s

that for loop can't be paralyzed well I
00:22:51.169
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m51s

say that there's a recent thing called
00:22:57.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h22m57s

QR an N which is will hopefully talk
00:23:00.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m00s

about in part two which kind of does
00:23:03.139
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m03s

paralyze it but the versions we're
00:23:04.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m04s

looking at don't paralyze it so there
00:23:05.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m05s

would be the main issues I think before
00:23:08.419
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m08s

look at performance look at memory and
00:23:09.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m09s

look at stability and try and find a
00:23:12.169
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m12s

number that's you know as high as you
00:23:14.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m14s

can make it but all of those things work
00:23:17.149
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m17s

for you
00:23:19.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m19s

okay so trying to get although that
00:23:23.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m23s

chunking and lining up and anything to
00:23:29.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m29s

work is more code than I want to write
00:23:31.299
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m31s

so for this section we're going to go
00:23:33.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m33s

back and use torched s together okay
00:23:35.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m35s

so when you're using AP is like fast AI
00:23:37.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m37s

and torch text which in these case these
00:23:44.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m44s

two API is a desire to or at least from
00:23:46.809
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m46s

the first day I site designed to work
00:23:48.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m48s

together you often have a choice which
00:23:50.289
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m50s

is like okay this API has a number of
00:23:52.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m52s

methods that expect the data in this
00:23:55.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m55s

kind of format and you can either change
00:23:58.299
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h23m58s

your data to fit that format or you can
00:24:01.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m01s

write your own data set subclass to
00:24:03.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m03s

handle the format that your data is
00:24:07.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m07s

already in I've noticed on the forum a
00:24:08.919
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m08s

lot of you are spending a lot of time
00:24:12.809
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m12s

writing your own dataset classes whereas
00:24:14.919
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m14s

I am way lazier than you and I spend my
00:24:17.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m17s

time instead changing my data to fit the
00:24:20.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m20s

data set classes I have like I that's
00:24:23.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m23s

fine and if you realize like oh there's
00:24:27.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m27s

a kind of a format of data that me and
00:24:31.419
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m31s

other people are likely to be seen quite
00:24:34.539
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m34s

often and it's not in the first day our
00:24:36.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m36s

library then by all means write the data
00:24:37.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m37s

set subclass it submitted as a PR and
00:24:40.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m40s

then everybody can benefit you know but
00:24:42.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m42s

in this case I just kind of thought I
00:24:45.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m45s

want to have some Nietzsche data fed
00:24:48.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m48s

into torch text I'm just going to put it
00:24:52.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m52s

in the format that watch text kind of
00:24:55.869
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m55s

already support so torch text already
00:24:57.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h24m57s

has or at least the first day I wrap her
00:25:00.249
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m00s

around Twitter text already has
00:25:02.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m02s

something where you can have a training
00:25:04.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m04s

path and a validation path and you know
00:25:05.529
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m05s

one or more text files in each path
00:25:09.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m09s

containing a bunch of stuff that's
00:25:11.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m11s

concatenated together for your language
00:25:12.999
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m12s

model so in this case all I did was I
00:25:14.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m14s

made a copy of my nature file copied it
00:25:17.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m17s

into training made another copy stuck it
00:25:21.249
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m21s

into the validation and then in one of
00:25:23.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m23s

the you know in the training set I did I
00:25:26.019
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m26s

deleted the last twenty percent of rows
00:25:29.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m29s

and in the validation set I deleted all
00:25:32.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m32s

except for the last one
00:25:34.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m34s

Center for us and I was done right so I
00:25:35.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m35s

found this that in this case I found
00:25:39.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m39s

that easier than writing a custom
00:25:40.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m40s

dataset class the other benefit of doing
00:25:43.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m43s

it that way was that I felt like it was
00:25:45.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m45s

more realistic to have a validation set
00:25:47.919
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m47s

that wasn't a random shuffled set of
00:25:50.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m50s

rows of text that was like a totally
00:25:53.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m53s

separate part of the corpus because I
00:25:56.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m56s

feel like in practice you're very often
00:25:59.379
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h25m59s

going to be saying like oh I've got I
00:26:01.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m01s

don't know these books or these authors
00:26:02.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m02s

I'm learning from and then I want to
00:26:05.799
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m05s

apply it to these different books and
00:26:07.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m07s

these different authors you know so I
00:26:08.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m08s

felt like for getting a more realistic
00:26:10.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m10s

validation of my nietzsche model I
00:26:13.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m13s

should use like a whole separate piece
00:26:15.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m15s

of the text so in this case it's the
00:26:18.519
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m18s

last you know 20% of the rows if the
00:26:20.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m20s

corpus so I haven't created this for you
00:26:23.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m23s

right intentionally because you know
00:26:28.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m28s

this is the kind of stuff I want to do
00:26:31.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m31s

practicing is making sure that you're
00:26:32.529
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m32s

familiar enough comfortable enough with
00:26:35.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m35s

with batch or whatever that you can
00:26:36.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m36s

create these and that you understand
00:26:38.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m38s

what they need to look like and so forth
00:26:40.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m40s

so in this case you can see I've now got
00:26:42.629
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m42s

you know a train and a validation here
00:26:48.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m48s

and then I could yeah okay so you can
00:26:51.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m51s

see I've literally just got one file in
00:26:57.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m57s

it because it's a fire when you're doing
00:26:59.169
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h26m59s

a language model ie predicting the next
00:27:00.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m00s

character or predicting the next word
00:27:02.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m02s

you don't really need separate files
00:27:04.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m04s

it's fine if you do have separate files
00:27:07.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m07s

but they just get capped native together
00:27:09.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m09s

anyway alright so that's my source data
00:27:11.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m11s

and so here is you know the same lines
00:27:15.419
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m15s

of code that we've seen before and let's
00:27:19.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m19s

go over them again so it's a couple of
00:27:20.919
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m20s

lessons ago right so in torch text we
00:27:22.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m22s

create this thing called a field and the
00:27:25.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m25s

field initially is just a description of
00:27:28.269
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m28s

how to go about pre-processing the test
00:27:31.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m31s

okay now you in this case I'm gonna say
00:27:36.129
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m36s

hey lowercase it you know cuz I don't
00:27:38.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m38s

mean now I think about it there's no
00:27:41.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m41s

particular reason to have done this
00:27:44.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m44s

lower case upper case would work fine
00:27:45.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m45s

too and then how do I talk
00:27:46.629
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m46s

maser and so you might remember last
00:27:49.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m49s

time we used a tokenization function
00:27:50.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m50s

which kind of largely spit on white
00:27:53.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m53s

space and try to do some flavor things
00:27:56.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m56s

with punctuation right and that gave us
00:27:57.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m57s

a word model in this case I want to
00:27:59.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h27m59s

character model so I actually want every
00:28:02.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m02s

character put into a separate token so I
00:28:04.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m04s

could just use the function list in
00:28:07.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m07s

Python because list in Python does that
00:28:10.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m10s

okay so this is where you can kind of
00:28:16.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m16s

see like understanding how libraries
00:28:19.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m19s

like torch text and fast ar e are
00:28:23.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m23s

designed to be extended can make your
00:28:25.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m25s

life a lot easier right so when you
00:28:27.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m27s

realize that very often both of these
00:28:30.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m30s

libraries kind of expect you to pass a
00:28:33.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m33s

function that does something and then
00:28:35.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m35s

you realize like oh I can write any
00:28:38.809
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m38s

function I like all right okay so this
00:28:40.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m40s

is now going to mean that each mini
00:28:44.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m44s

batch is going to contain a list of
00:28:47.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m47s

characters and so here's where we get to
00:28:49.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m49s

define all our different parameters and
00:28:52.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m52s

so to make it the same as previous
00:28:55.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m55s

sections of this notebook I'm going to
00:28:58.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h28m58s

use the same batch size the same number
00:29:00.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m00s

of characters then they're going to
00:29:03.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m03s

rename it to their PT t since we know
00:29:04.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m04s

what that means
00:29:06.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m06s

the number of the size of the embedding
00:29:08.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m08s

and the size of our hidden state okay
00:29:12.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m12s

remembering that size of our hidden
00:29:14.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m14s

state simply means going all the way
00:29:16.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m16s

back to the start right and hidden
00:29:21.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m21s

simply means the size of the state
00:29:25.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m25s

that's created by each of those orange
00:29:28.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m28s

arrows so it's the size of each of those
00:29:29.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m29s

circles yeah okay
00:29:31.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m31s

so having done that we can then create a
00:29:38.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m38s

little dictionary saying what's our
00:29:40.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m40s

training validation and test set in this
00:29:42.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m42s

case I don't have a separate test set so
00:29:45.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m45s

I just use the same thing and then I can
00:29:46.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m46s

say all right I want a language model
00:29:50.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m50s

data subclass with model data I'm going
00:29:51.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m51s

to grab it from text files and this is
00:29:55.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m55s

my path and this is my field which I
00:29:58.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h29m58s

defined earlier and these are my files
00:30:03.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m03s

and these are my hyper parameters min
00:30:06.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m06s

fracks not going to do anything actually
00:30:11.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m11s

in this case because there's not I don't
00:30:12.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m12s

think there's going to be any character
00:30:14.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m14s

that appears less than three times
00:30:15.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m15s

that's probably redundant okay so at the
00:30:17.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m17s

end of that it says there's going to be
00:30:22.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m22s

963 batches to go through and so if you
00:30:24.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m24s

think about it that should be equal to
00:30:29.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m29s

the number of tokens divided by the
00:30:31.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m31s

batch size divided by B PTT because
00:30:35.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m35s

that's like the size of each of those
00:30:38.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m38s

rectangles
00:30:40.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m40s

you'll find that in practice it's not
00:30:45.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m45s

exactly that and the reason it's not
00:30:47.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m47s

exactly that is that the authors of
00:30:49.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m49s

torch text did something pretty smart
00:30:52.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m52s

which I think we've briefly mentioned
00:30:56.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m56s

this before they said okay we can't
00:30:57.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m57s

shuffle the data like with images we'd
00:30:59.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h30m59s

like to shuffle the order so every time
00:31:02.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m02s

we see them in a different order so
00:31:03.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m03s

there's a bit more random listed we
00:31:04.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m04s

can't shuffle because we need to be
00:31:06.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m06s

contiguous but what we could do is
00:31:08.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m08s

randomize the length of you know
00:31:11.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m11s

basically randomize be PTT a little bit
00:31:13.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m13s

each time and so that's what PI torch
00:31:16.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m16s

does it's not always going to give us
00:31:19.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m19s

exactly 8 characters long 5% of the time
00:31:21.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m21s

it'll actually cut it enough and then
00:31:26.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m26s

it's going to add on a small little
00:31:30.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m30s

standard deviation you know to make it
00:31:32.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m32s

slightly bigger or smaller than for
00:31:35.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m35s

white okay so it's going to be slightly
00:31:36.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m36s

different to eight on average
00:31:38.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m38s

yes just to make sure is it going to be
00:31:45.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m45s

constant per Vinny watch yeah yeah
00:31:50.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m50s

exactly that's right so a mini batch you
00:31:53.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m53s

know has to kind of it needs to do a
00:31:57.039
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h31m57s

matrix multiplication and the mini batch
00:32:01.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m01s

size has to remain constant because
00:32:06.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m06s

we've got this age weight matrix that
00:32:09.279
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m09s

has to you know has to line up in size
00:32:14.289
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m14s

with the size of the mini batch yeah but
00:32:16.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m16s

the number you know the sequence length
00:32:20.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m20s

can can change their problem okay so
00:32:23.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m23s

that's why we have 963 that's so the
00:32:33.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m33s

length of a data loader is how many mini
00:32:36.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m36s

batches in this case it's so do it
00:32:37.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m37s

approximate okay number of tokens is how
00:32:39.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m39s

many unique things are in the vocabulary
00:32:42.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m42s

and remember after we run this line text
00:32:45.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m45s

now does not just contain in a
00:32:51.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m51s

description of what we want but it also
00:32:54.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m54s

contains an extra attribute called vocab
00:32:56.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h32m56s

right which contains stuff like a list
00:33:01.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m01s

of all of the unique items in the
00:33:06.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m06s

vocabulary and a reverse mapping from
00:33:10.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m10s

each item to its number okay so that
00:33:14.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m14s

text object is now an important thing to
00:33:19.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m19s

keep out all right so let's now try this
00:33:23.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m23s

so we do we started out by looking at
00:33:31.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m31s

the class so the class is exactly the
00:33:34.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m34s

same as the class we've had before the
00:33:36.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m36s

only key difference is to call in at
00:33:39.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m39s

hidden which calls sets out so H is not
00:33:41.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m41s

a variable anymore it's now an attribute
00:33:44.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m44s

itself that H is a variable containing a
00:33:46.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m46s

bunch of zeros now I've been shown that
00:33:50.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m50s

that size remains constant H time
00:33:54.779
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m54s

but unfortunately when I said that I
00:33:58.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h33m58s

lied to you and the way that I lied to
00:34:00.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m00s

you is that the very last mini batch
00:34:03.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m03s

will be shorter okay the very last mini
00:34:07.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m07s

batch is actually going to have less
00:34:10.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m10s

than 60 well it might be exactly the
00:34:12.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m12s

right size if it so happens that this
00:34:14.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m14s

data set is exactly divisible by B PTT
00:34:16.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m16s

times patch size but it probably isn't
00:34:19.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m19s

so the last batch will probably has a
00:34:21.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m21s

little bit less okay and so that's why I
00:34:23.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m23s

do a little check here that says let's
00:34:27.169
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m27s

check that the batch size inside self
00:34:29.419
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m29s

dot H right and so self dot H is going
00:34:31.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m31s

to be the height sorry the height is
00:34:35.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m35s

going to be the number of activations
00:34:40.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m40s

and the width is going to be the mini
00:34:44.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m44s

batch size okay check that that's equal
00:34:46.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m46s

to the actual sequence length sorry the
00:34:49.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m49s

actual batch size length that we've
00:34:54.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m54s

received okay and if they're not the
00:34:56.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m56s

same then set it back to 0 's again okay
00:34:58.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h34m58s

so this is just a minor little wrinkle
00:35:02.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m02s

that basically at the end of each epoch
00:35:05.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m05s

it's going to do like a little mini mini
00:35:07.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m07s

batch right and so then as soon as it
00:35:10.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m10s

starts the next epoch it's going to see
00:35:13.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m13s

that they're not the same again and
00:35:15.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m15s

it'll reinitialize it to the correct
00:35:17.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m17s

full batch size okay so that's why you
00:35:19.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m19s

know if you're wondering there's an
00:35:21.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m21s

inert hidden not just in the constructor
00:35:23.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m23s

but also inside forward it's to handle
00:35:25.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m25s

this kind of end of each epoch start of
00:35:28.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m28s

each epoch difference okay not an
00:35:31.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m31s

important point by any means but
00:35:34.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m34s

potentially confusing when you see it
00:35:36.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m36s

okay so the last wrinkle the last
00:35:39.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m39s

wrinkle is something which i think is
00:35:47.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m47s

something that slightly sucks about pi
00:35:51.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m51s

torch and maybe somebody can be nice
00:35:53.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m53s

enough to try and fix it with a PR if
00:35:55.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m55s

anybody feels like it which is that the
00:35:57.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m57s

loss functions such as softmax
00:35:59.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h35m59s

I'm not happy receiving a rank 3 tensor
00:36:04.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m04s

remember a rank 3 tensor is just another
00:36:09.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m09s

way of saying
00:36:11.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m11s

dimension three right okay there's no
00:36:12.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m12s

particular reason they ought to not be
00:36:17.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m17s

happy receiving a rank 3 tensor you know
00:36:18.799
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m18s

like somebody could write some code to
00:36:21.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m21s

say hey a wreck three tensor is probably
00:36:22.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m22s

you know a sequence length by batch size
00:36:24.559
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m24s

by you know results thing and so you
00:36:27.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m27s

should just do it for each of the two
00:36:31.579
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m31s

initial axis but no one's done that and
00:36:35.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m35s

so it expects it to be a rank two tensor
00:36:39.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m39s

funnily enough it can handle write to or
00:36:43.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m43s

rec for they're not right through yeah
00:36:45.799
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m45s

so we've got so we've got a rank two
00:36:48.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h36m48s

tensor containing you know for each time
00:37:00.619
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m00s

period I can't remember which way around
00:37:04.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m04s

the the y axes are but whatever for each
00:37:08.359
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m08s

time period for each batch we've got our
00:37:11.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m11s

predictions okay and then we've got our
00:37:17.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m17s

our actuals for each time period for
00:37:21.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m21s

each batch we've got our predictions and
00:37:27.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m27s

we've got our actuals okay and so we
00:37:31.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m31s

just want to check whether they're the
00:37:34.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m34s

same and so at an ideal world our lost
00:37:35.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m35s

function a loss function would check you
00:37:37.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m37s

know item 1 1 then item 1 2 and then
00:37:39.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m39s

item 1 3 but since that hasn't been
00:37:42.319
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m42s

written we just have to flatten them
00:37:44.299
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m44s

both out okay and we can literally just
00:37:46.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m46s

flatten them out put rose - rose and so
00:37:48.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m48s

that's why here I have to use dot view
00:37:53.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m53s

okay and so dot view says the number of
00:37:57.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h37m57s

columns will be equal to the size of the
00:38:04.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m04s

vocab because remember we're going to
00:38:06.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m06s

end up with a prediction you know a
00:38:07.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m07s

probability for each letter and then the
00:38:08.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m08s

number of rows is however big is
00:38:11.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m11s

necessary which will be equal to batch
00:38:14.359
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m14s

size times B PTT okay and then you may
00:38:15.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m15s

be wondering where I do that
00:38:24.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m24s

that's so that's where the predictions
00:38:26.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m26s

you may be wondering where I do that for
00:38:27.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m27s

the target and the answer is torch text
00:38:28.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m28s

knows that the target needs to look like
00:38:31.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m31s

that
00:38:32.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m32s

so torch text has already done that for
00:38:33.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m33s

us okay so torch text automatically
00:38:34.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m34s

changes the target to be flattened out
00:38:36.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m36s

as you might actually remember if you go
00:38:39.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m39s

back to lesson 4 when we actually looked
00:38:41.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m41s

at a mini batch that's bad out of torch
00:38:45.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m45s

text we did we noticed actually that it
00:38:48.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m48s

was flattened and I said we'll learn
00:38:50.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m50s

about why later and so later is now all
00:38:52.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m52s

right okay so they're the three wrinkles
00:38:55.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h38m55s

get rid of the history ooh I guess for
00:39:00.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m00s

wrinkles recreate the hidden state if
00:39:06.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m06s

the batch size changes flatten out and
00:39:11.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m11s

then you just torch text to create mini
00:39:16.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m16s

batches that line up nicely so once we
00:39:19.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m19s

do those things we can then create our
00:39:23.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m23s

model create our optimizer with that
00:39:26.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m26s

models parameters and fit it one thing
00:39:30.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m30s

to be careful of here is that softmax
00:39:38.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m38s

now as of hi torch 0.3 requires that we
00:39:48.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m48s

pass in a number here saying which
00:39:55.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m55s

access do we want to do the softmax over
00:39:59.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h39m59s

so at this point this is a
00:40:02.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m02s

three-dimensional tensor right and so we
00:40:05.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m05s

want to do the softmax over the final
00:40:08.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m08s

axis right so when i say which axis do
00:40:10.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m10s

we do the softmax over remember we
00:40:13.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m13s

divide by there we go e to the X I
00:40:15.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m15s

divided by the sum of e to the X I so
00:40:18.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m18s

it's saying which axis do we sum over so
00:40:21.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m21s

which access we want to sum to one and
00:40:24.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m24s

so in this case clearly we want to do it
00:40:26.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m26s

over the last axis because the last axis
00:40:29.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m29s

is the one that contains the probability
00:40:31.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m31s

per letter of the alphabet and we want
00:40:33.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m33s

all of those probabilities to sum to one
00:40:36.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m36s

okay
00:40:37.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m37s

so therefore to run this notebook you're
00:40:40.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m40s

going to need PI torch 0.3 which just
00:40:45.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m45s

came out this week ok so if you're doing
00:40:48.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m48s

this on the milk you're fine I'm sure
00:40:50.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m50s

you've got at least a 0.3 or later ok
00:40:51.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m51s

where else the students here if you just
00:40:54.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m54s

go Conda and update it will
00:40:56.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m56s

automatically update you to 0.3 the
00:40:58.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h40m58s

really great news is that 0.3 although
00:41:02.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m02s

it does not yet officially support
00:41:06.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m06s

windows it does in practice I
00:41:08.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m08s

successfully installed 0.3 from Condor
00:41:11.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m11s

yesterday by typing Condor install torch
00:41:14.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m14s

PI torch in Windows are then attempted
00:41:16.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m16s

to use the entirety of lesson 1 and
00:41:19.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m19s

every single part worked so I actually
00:41:21.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m21s

ran it on this very laptop so for those
00:41:24.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m24s

who are interested in doing deep burning
00:41:27.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m27s

on their laptop can definitely recommend
00:41:30.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m30s

the new surface book the new surface
00:41:32.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m32s

book 15-inch has a gtx 1066 gig GPU in
00:41:36.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m36s

it and i was getting and it was running
00:41:42.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m42s

about 3 times slower than my 1080 TI
00:41:45.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m45s

which i think means it's about the same
00:41:53.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m53s

speed as an AW sp2 the instance and as
00:41:55.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h41m55s

you can see it's also a nice convertible
00:42:00.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m00s

tablet that you can write on and it's
00:42:02.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m02s

thin and light and so it's like I've
00:42:04.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m04s

never seen a such a good deep learning
00:42:07.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m07s

boss also I successfully installed Linux
00:42:09.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m09s

on eart and all of the faster I staff
00:42:13.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m13s

worked on Linux as well so really good
00:42:16.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m16s

option if you're interested in a laptop
00:42:18.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m18s

that can run deep learning stuff
00:42:22.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m22s

[Music]
00:42:25.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m25s

alright so that's that's going to be
00:42:26.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m26s

aware of with this team equals minus 1
00:42:29.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m29s

so then we can go ahead and construct
00:42:30.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m30s

this and we can call fit and yeah we're
00:42:33.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m33s

basically going to get pretty similar
00:42:36.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m36s

results to what we got the ball
00:42:39.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m39s

alright so then we can go a bit further
00:42:43.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m43s

without RNN by just kind of unpacking it
00:42:49.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m49s

a bit more
00:42:52.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m52s

and so this is now again exactly the
00:42:53.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m53s

same thing gives exactly the same
00:42:55.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m55s

answers but I have removed the cold air
00:42:57.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h42m57s

in it so I've got rid of this self to
00:43:02.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m02s

iron in okay and so this is just
00:43:05.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m05s

something I won't spend time on it but
00:43:09.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m09s

you can check it out
00:43:10.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m10s

so instead I've now to find iron in as R
00:43:11.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m11s

and n cell and I've copied and pasted
00:43:14.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m14s

the code above don't run it this is just
00:43:17.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m17s

for your reference from PI torch this is
00:43:19.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m19s

this is the color the definition of
00:43:22.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m22s

eridan so in PI torch and I want you to
00:43:24.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m24s

see that you can now read PI torch
00:43:26.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m26s

source code and understand it not only
00:43:29.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m29s

that you'll recognize it as being
00:43:32.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m32s

something we've done before it's a
00:43:33.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m33s

matrix modification of the weights by
00:43:35.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m35s

the inputs plus biases so f dot linear
00:43:38.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m38s

simply does a matrix product followed by
00:43:42.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m42s

an addition right and interestingly
00:43:45.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m45s

you'll see they do not concatenate the
00:43:47.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m47s

the input bit and the hidden bit they
00:43:51.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m51s

sum them together which is our first
00:43:55.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m55s

approach and I'm as I said you can do
00:43:58.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h43m58s

either neither one's right or wrong but
00:44:00.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m00s

it's interesting to see this is the
00:44:02.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m02s

definition yeah
00:44:03.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m03s

yes you know can give some insight about
00:44:05.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m05s

what are they using that particular
00:44:10.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m10s

activation function firm yeah yeah I
00:44:12.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m12s

think you might have briefly covered
00:44:18.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m18s

this last week but very happy to do it
00:44:20.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m20s

again if I did basically fan that's
00:44:22.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m22s

positive 1 and negative 1
00:44:28.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m28s

then looks like that so in other words
00:44:36.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m36s

it's a sigmoid function double the
00:44:39.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m39s

height minus one naturally they're
00:44:42.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m42s

they're equal so it's it's it's a nice
00:44:45.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m45s

function in that it's forcing it to be
00:44:49.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m49s

you know no smaller than minus one no
00:44:53.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m53s

bigger than plus one and since we're
00:44:55.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m55s

multiplying by this white matrix again
00:44:57.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m57s

and again and again and again we might
00:44:59.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h44m59s

worry that our Lu because it's unbounded
00:45:03.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m03s

might have more of a gradient explosion
00:45:05.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m05s

problem that's basically the theory
00:45:08.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m08s

having said that you can actually ask pi
00:45:11.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m11s

torch for an RNN cell which uses a
00:45:19.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m19s

different non-linearity so you can see
00:45:24.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m24s

by default it uses then you can ask for
00:45:26.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m26s

a value as well but yeah most people
00:45:28.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m28s

seem to pretty much everybody still
00:45:31.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m31s

seems to use them as far as I can tell
00:45:32.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m32s

so you can basically see here this is
00:45:36.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m36s

all the same except now I've got an
00:45:38.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m38s

errand in cell which means now I need to
00:45:40.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m40s

put my for loop back alright and you can
00:45:41.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m41s

see every time I call my my little
00:45:44.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m44s

linear function I just obtained the
00:45:48.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m48s

result onto my list okay and at the end
00:45:51.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m51s

the result is that all stacked up
00:45:54.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m54s

together okay so like just trying to
00:45:57.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h45m57s

show you how nothing inside PI torches
00:46:00.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m00s

is mysterious right you should find you
00:46:04.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m04s

get basically the fact you I found I got
00:46:06.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m06s

exactly the same answer from this as the
00:46:08.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m08s

previous one okay in practice you would
00:46:11.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m11s

never write it like this but what you
00:46:14.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m14s

may well find in practice is that
00:46:15.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m15s

somebody will come up with like a new
00:46:17.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m17s

kind of eridan cell or a different way
00:46:19.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m19s

of kind of keeping track of things over
00:46:22.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m22s

time or a different way of doing
00:46:24.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m24s

regularization and so inside posterize
00:46:25.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m25s

code you will find that we we do this
00:46:30.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m30s

exactly this basically we have this by
00:46:34.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m34s

hand because we use some regularization
00:46:37.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m37s

approaches that are supported by pi
00:46:39.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m39s

torch
00:46:41.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m41s

all right so then another thing I'm not
00:46:43.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m43s

going to spend much time on but I'll
00:46:46.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m46s

mention briefly is that nobody really
00:46:47.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m47s

uses this hour an insult in practice and
00:46:50.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m50s

the reason we don't use that eridan so
00:46:54.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m54s

in practice is even though the fan is
00:46:56.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m56s

here you do tend to find gradient
00:46:58.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h46m58s

explosions are still a problem and so we
00:47:03.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m03s

have to use pretty low learning rates to
00:47:05.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m05s

get these to train and pretty small
00:47:08.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m08s

values for B PTT to get them to Train so
00:47:10.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m10s

what we do instead is we replace the
00:47:15.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m15s

eridan cell with something like this
00:47:17.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m17s

this is called a GI u cell and a GI u so
00:47:20.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m20s

here it is has a picture of it and
00:47:29.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m29s

there's the equations for it so
00:47:35.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m35s

basically I'll show you both quickly but
00:47:38.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m38s

we'll talk about it much more in part
00:47:41.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m41s

two we've got our input okay
00:47:42.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m42s

and our input normally goes straight in
00:47:47.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m47s

gets multiplied by a weight matrix to
00:47:51.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m51s

create our new activations that's not
00:47:56.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h47m56s

what happens
00:48:00.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m00s

and then we've cost we also we add it to
00:48:02.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m02s

the existing activations that's not what
00:48:04.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m04s

happens here in this case our input goes
00:48:06.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m06s

into this H tilde temporary thing and it
00:48:10.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m10s

doesn't just get added to our
00:48:14.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m14s

activations their previous activations
00:48:16.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m16s

that our previous activations get
00:48:18.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m18s

multiplied by this value R and R stands
00:48:20.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m20s

for reset it's a reset gate okay and how
00:48:24.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m24s

do we calculate that this this value
00:48:29.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m29s

goes between Norton one right in our
00:48:31.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m31s

reset gate well the answer is it's
00:48:33.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m33s

simply equal to a matrix product between
00:48:36.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m36s

some weight matrix and the concatenation
00:48:40.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m40s

of our previous hidden state and our new
00:48:43.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m43s

input in other words this is a little
00:48:46.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m46s

one hidden layer neural net and in
00:48:49.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m49s

particular it's a one hidden layer
00:48:53.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m53s

neural net because we're then put it
00:48:54.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m54s

through the sigmoid function
00:48:56.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m56s

when you seasick but one of the things I
00:48:57.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m57s

hate about mathematical notation is
00:48:59.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h48m59s

symbols are overloaded a lot right
00:49:00.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m00s

sometimes when you see Sigma that means
00:49:03.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m03s

standard deviation when you see it next
00:49:05.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m05s

to a parenthesis like this it means the
00:49:07.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m07s

sigmoid function okay so in other words
00:49:09.529
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m09s

that okay which looks like that okay so
00:49:16.839
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m16s

this is like a little mini neural net
00:49:26.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m26s

with no hidden layers or to think of it
00:49:28.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m28s

another way it's like a little logistic
00:49:29.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m29s

regression okay and this is I mentioned
00:49:30.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m30s

this briefly because it's going to come
00:49:34.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m34s

up a lot in part two and so it's a good
00:49:35.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m35s

thing to like start learning about it's
00:49:37.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m37s

this idea that like in the very learning
00:49:39.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m39s

itself you can have like little mini
00:49:43.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m43s

neural nets inside your neural nets
00:49:45.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m45s

right and so this little mini neural net
00:49:47.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m47s

is going to be used to decide how much
00:49:50.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m50s

of my hidden state am I going to
00:49:53.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m53s

remember right and so it might learn
00:49:56.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m56s

that Oh in this particular situation
00:49:58.279
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h49m58s

forget everything you know for example
00:50:00.579
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m00s

oh there's a full-stop you know hey when
00:50:03.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m03s

you see a full-stop you should throw
00:50:05.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m05s

away nearly all of your hidden state
00:50:07.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m07s

that is probably something it would
00:50:09.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m09s

learn and that that's very easy for it
00:50:11.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m11s

to learn using this little mini neuron
00:50:12.859
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m12s

there okay and so that goes through to
00:50:14.839
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m14s

create my new hidden state along with
00:50:17.119
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m17s

the input and then there's a second
00:50:20.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m20s

thing that happens which is there's this
00:50:22.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m22s

gate here called Z and what Z says is
00:50:24.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m24s

all right you've got your some amount of
00:50:28.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m28s

your previous hidden state plus your new
00:50:31.759
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m31s

input right and it's going to go through
00:50:34.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m34s

to create your new state and I'm going
00:50:36.559
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m36s

to let you decide to what degree do you
00:50:39.259
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m39s

use this new input version of your
00:50:41.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m41s

hidden state and to what degree where
00:50:45.799
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m45s

you just leave the hidden state the same
00:50:47.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m47s

as before so this thing here is called
00:50:49.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m49s

the update gate right and so it's got
00:50:50.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m50s

two choices it can make the first-years
00:50:53.809
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m53s

to throw away some hidden state when
00:50:55.519
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m55s

deciding how much to incorporate that
00:50:57.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m57s

versus my new input and how much to
00:50:59.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h50m59s

update my hidden state versus just leave
00:51:02.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m02s

it exactly the same and the equation
00:51:05.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m05s

hopefully is going to look pretty
00:51:08.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m08s

similar
00:51:10.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m10s

to you which is check this out here
00:51:11.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m11s

remember how I said you want to start to
00:51:14.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m14s

recognize some some common ways of
00:51:16.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m16s

looking at things
00:51:19.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m19s

well here I have a 1 - something by a
00:51:20.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m20s

thing and a something without the 1 - by
00:51:25.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m25s

a thing which remember is a linear
00:51:30.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m30s

interpolation right so in other words
00:51:33.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m33s

the value of Z is going to decide to
00:51:35.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m35s

what degree do I have keep the previous
00:51:39.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m39s

hidden state and to what degree do I use
00:51:43.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m43s

the new hidden state right so that's why
00:51:46.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m46s

they draw it here as this kind of like
00:51:49.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m49s

it's not actually a switch but like you
00:51:52.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m52s

can put it in any position you can be
00:51:55.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m55s

like oh it's here or it's here or it's
00:51:57.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m57s

here to decide how much that'll do ok so
00:51:59.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h51m59s

so they're basically the equations it's
00:52:03.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m03s

a it's a little mini neural net with its
00:52:06.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m06s

own weight matrix to decide how much to
00:52:07.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m07s

update little mini neural net with its
00:52:09.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m09s

own weight matrix to decide how much to
00:52:11.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m11s

reset and then that's used to do an
00:52:13.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m13s

interpolation between the two hidden
00:52:15.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m15s

states so that's called giu
00:52:17.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m17s

gated recurrent Network there's the
00:52:21.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m21s

definition from the PI torch source code
00:52:24.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m24s

they have some slight optimizations here
00:52:27.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m27s

that if you're interested in we can talk
00:52:31.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m31s

about them on the forum but it's exactly
00:52:32.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m32s

the same for we just saw and so if you
00:52:35.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m35s

go and NDI you that it uses this same
00:52:39.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m39s

code but it replaces the iron in so with
00:52:43.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m43s

this cell okay and as a result rather
00:52:47.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m47s

than having something where we needed
00:52:51.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m51s

where we were getting a 1.54 we're now
00:52:53.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m53s

getting down to one point 400 and we can
00:52:58.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h52m58s

keep training even more get right down
00:53:02.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m02s

to one point three six okay so in
00:53:03.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m03s

practice a giu or very nearly
00:53:05.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m05s

equivalently we'll see in a moment in
00:53:08.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m08s

lsdm in practice what pretty much
00:53:09.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m09s

everybody always uses so the art he and
00:53:12.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m12s

HT are ultimately scalars after they go
00:53:18.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m18s

through the sigmoid but there are
00:53:22.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m22s

Clyde element-wise is that correct -
00:53:24.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m24s

mm-hmm
00:53:28.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m28s

yeah although of course one for each
00:53:29.349
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m29s

mini batch but yes the scaler yeah okay
00:53:31.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m31s

great thanks and on the excellent colas
00:53:36.859
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m36s

blog Chris Ellis blog there's an
00:53:44.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m44s

understanding LS TM networks post which
00:53:47.119
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m47s

you can read all about this in much more
00:53:50.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m50s

detail if you're interested and also the
00:53:52.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m52s

other one I was dealing from here is a
00:53:55.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m55s

wild ml also have a good blog post on
00:53:57.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m57s

this there's somebody wants to be
00:53:59.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h53m59s

helpful feel free to put them in the
00:54:01.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m01s

lesson wiki
00:54:02.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m02s

okay so then putting it all together I'm
00:54:08.319
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m08s

now going to replace my GI you with
00:54:13.069
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m13s

Nellis TM I'm not going to bother
00:54:15.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m15s

showing you the soul for this it's very
00:54:16.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m16s

similar to GI u but the LS TM has one
00:54:18.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m18s

more piece of state in it called the
00:54:21.829
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m21s

sell state not just the hidden state so
00:54:24.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m24s

if you do use an LS TM you're now inside
00:54:27.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m27s

you're in it hidden have to return a
00:54:29.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m29s

couple of matrices they're exactly the
00:54:31.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m31s

same size as the hidden state but you
00:54:34.609
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m34s

just have to return the tupple okay the
00:54:37.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m37s

details don't matter too much but we can
00:54:40.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m40s

talk about it during the week if you're
00:54:43.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m43s

interested you know when you pass in you
00:54:44.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m44s

still pass in self dot H still returns a
00:54:48.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m48s

new value of H is to repackage it in the
00:54:50.869
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m50s

usual way
00:54:53.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m53s

so this code is identical to the code
00:54:53.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m53s

before one thing I've done though is
00:54:55.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m55s

I've had a drop out inside my air and in
00:54:58.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h54m58s

which you can do with the height watch R
00:55:02.059
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m02s

and n function so that's going to do
00:55:05.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m05s

drop out after a time step and I've
00:55:07.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m07s

doubled the size of my hidden layer
00:55:09.829
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m09s

since I've now added point five drop out
00:55:11.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m11s

and so my hope was that this would make
00:55:13.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m13s

it be able to learn more but be more
00:55:15.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m15s

resilient as it does so so then I wanted
00:55:19.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m19s

to show you how to take advantage of a
00:55:24.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m24s

little bit more fast AI magic without
00:55:29.359
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m29s

using the layer class and so I'm going
00:55:32.859
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m32s

to show you how to use Cobra
00:55:35.839
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m35s

and specifically we're going to do s GDR
00:55:39.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m39s

without without using the learner class
00:55:44.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m44s

ok so to do that we create our model
00:55:46.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m46s

again just a standard PI torch model ok
00:55:49.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m49s

and this time rather than going
00:55:52.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m52s

remember the usual pipe torch approach
00:55:54.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m54s

is opt equals up to M naught atom and
00:55:56.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h55m56s

you pass in the parameters and a
00:56:00.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m00s

learning rate I'm not going to do that
00:56:01.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m01s

I'm going to use the fast AI layer
00:56:03.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m03s

optimizer class which takes my opt-in
00:56:06.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m06s

class constructor from PI torch it takes
00:56:12.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m12s

my model it takes my learning rate and
00:56:16.609
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m16s

optionally takes weight decay ok and so
00:56:20.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m20s

this class is tiny it doesn't do very
00:56:25.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m25s

much at all the key reason it exists is
00:56:28.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m28s

to do differential learning rates and
00:56:31.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m31s

differential weight decay right but the
00:56:33.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m33s

reason we need to use it is that all of
00:56:36.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m36s

the mechanics inside fast AI assumes
00:56:39.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m39s

that you have one of these right so if
00:56:41.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m41s

you want to use like callbacks or SPDR
00:56:43.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m43s

or whatever in code where you're not
00:56:46.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m46s

using the learner class then you need to
00:56:49.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m49s

use rather than saying you know opt
00:56:52.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m52s

equals off to m dot atom and here's my
00:56:54.859
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m54s

parameters you instead say lay optimizer
00:56:56.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h56m56s

okay
00:57:00.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m00s

so that gives us a layer optimizer
00:57:02.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m02s

object and if you're interested
00:57:06.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m06s

basically behind the scenes you can now
00:57:08.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m08s

grab a dot opt property which actually
00:57:12.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m12s

gives you the optimizer but you don't
00:57:18.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m18s

have to worry about that itself but
00:57:20.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m20s

that's basically what happens behind the
00:57:21.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m21s

scenes the key thing we can now do is
00:57:23.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m23s

that we can now when we call fit we can
00:57:25.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m25s

pass in that optimizer and we can also
00:57:29.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m29s

pass in some callbacks and specifically
00:57:34.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m34s

we're going to use the cosine annealing
00:57:36.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m36s

callback okay and so the cosine
00:57:40.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m40s

annealing callback requires a layer
00:57:42.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m42s

optimizer object right and so what this
00:57:45.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m45s

is going to do is it's going to do
00:57:48.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m48s

cosine annealing by changing the
00:57:50.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m50s

learning
00:57:51.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m51s

right inside this object okay so the
00:57:52.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m52s

details aren't terribly important we can
00:57:57.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m57s

talk about them on the forum it's really
00:57:59.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h57m59s

the concept I wanted to get across here
00:58:01.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m01s

right which is that now that we've done
00:58:02.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m02s

this we can say all right
00:58:05.029
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m05s

create a cosine and kneeling callback
00:58:06.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m06s

which is going to update the learning
00:58:08.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m08s

rates in this layer optimizer the length
00:58:10.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m10s

of an epoch is equal to this here right
00:58:14.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m14s

how many mini batches are there in an
00:58:18.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m18s

epoch well it's whatever the length of
00:58:19.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m19s

this data loader is okay so because it's
00:58:22.039
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m22s

going to be it's going to be doing the
00:58:24.319
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m24s

cosine annealing it needs to know how
00:58:25.759
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m25s

often to reset okay and then you can
00:58:27.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m27s

pass in the cycle more in the usual way
00:58:31.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m31s

and then we can even save our model
00:58:33.589
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m33s

automatically like you remember how
00:58:37.999
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m37s

there was that cycle saved name
00:58:39.829
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m39s

parameter that we can pass to learn not
00:58:42.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m42s

fit this is what it does behind the
00:58:44.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m44s

scenes behind the scenes it sets an on
00:58:45.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m45s

cycle end callback and so here I have to
00:58:48.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m48s

find that callback as being something
00:58:51.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m51s

that saves my model okay so there's
00:58:53.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m53s

quite a lot of cool stuff that you can
00:58:57.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h58m57s

do with callbacks callbacks are
00:59:00.049
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m00s

basically things where you can define
00:59:02.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m02s

like at the start of training or at the
00:59:04.339
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m04s

start of an epoch or at the side of a
00:59:06.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m06s

batch or at the end of training or at
00:59:07.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m07s

the end of an epoch or at the end of a
00:59:09.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m09s

batch please call this club okay and so
00:59:10.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m10s

we've written some for you including SGD
00:59:13.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m13s

R which is the cosine annealing callback
00:59:17.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m17s

and then so how I recently wrote a new
00:59:20.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m20s

callback to implement the new approach
00:59:23.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m23s

to decoupled weight decay we use
00:59:26.089
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m26s

callbacks to draw those little graphs of
00:59:28.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m28s

the loss over time so there's lots of
00:59:31.579
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m31s

cool stuff you can do with callbacks so
00:59:34.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m34s

in this case by passing in that callback
00:59:36.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m36s

we're getting as JDR and that's able to
00:59:39.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m39s

get us down to one point three one here
00:59:43.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m43s

and then we can train a little bit more
00:59:48.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m48s

and eventually get down to one point two
00:59:49.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m49s

five and so we can now test that out and
00:59:54.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m54s

so if we passed in a few characters of
00:59:58.809
https://www.youtube.com/watch?v=H3g26EVADgY#t=00h59m58s

text we get not surprisingly and a
01:00:01.819
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m01s

after four thus let's do then 400 and
01:00:05.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m05s

now we have our own Nietzsche so
01:00:08.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m08s

Nietzsche tends to start his sections
01:00:12.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m12s

with a number and a dot so 2 9 3 perhaps
01:00:14.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m14s

that every life the values of blood have
01:00:17.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m17s

intercourse when it senses there is
01:00:20.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m20s

unscrupulous his very rights and still
01:00:22.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m22s

impulse love ok so I mean it's slightly
01:00:24.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m24s

less clear than Nietzsche normally but
01:00:27.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m27s

it gets the tone right ok and it's
01:00:29.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m29s

actually quite interesting like if to
01:00:33.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m33s

play around with training these
01:00:36.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m36s

character based language models to like
01:00:38.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m38s

run this at different levels of loss to
01:00:40.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m40s

get a sense of like what does it look
01:00:44.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m44s

like like you really notice that this is
01:00:45.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m45s

like 1.25 and like that's slightly worse
01:00:49.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m49s

like 1.3 this looks like total junk you
01:00:53.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m53s

know there's like punctuation in random
01:00:57.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m57s

places and you know nothing makes sense
01:00:59.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h00m59s

and like you start to realize that the
01:01:01.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m01s

difference between you know Nietzsche
01:01:03.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m03s

and random junk is not that far in kind
01:01:06.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m06s

of language model terms and so if you
01:01:09.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m09s

train this for a little bit longer
01:01:12.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m12s

you'll suddenly find like oh it's it's
01:01:13.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m13s

it's making more and more sense okay so
01:01:15.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m15s

if you are playing around with NLP stuff
01:01:18.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m18s

particularly generative stuff like this
01:01:21.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m21s

and you're like there is also like kind
01:01:23.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m23s

of okay but not great don't be
01:01:27.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m27s

disheartened because that means you're
01:01:29.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m29s

actually very very nearly there you know
01:01:31.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m31s

the the difference between like
01:01:33.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m33s

something which is starting to create
01:01:34.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m34s

something which almost vaguely looks
01:01:36.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m36s

English if you squint and something
01:01:38.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m38s

that's actually a very good generation
01:01:40.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m40s

it's it's not it's not far in most
01:01:42.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m42s

motion tests okay great so let's take a
01:01:45.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m45s

five-minute break we'll come back at
01:01:49.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m49s

7:45 and we're going to go back to
01:01:50.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m50s

computer vision
01:01:52.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m52s

okay so now become full circle back to
01:01:57.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h01m57s

vision so now we're looking at less than
01:02:04.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m04s

seven so far ten that book you might
01:02:10.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m10s

have heard of so far ten it's a really
01:02:18.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m18s

well-known data set in academia and it's
01:02:20.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m20s

partly it's well known it's actually
01:02:24.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m24s

pretty old by you know computer vision
01:02:26.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m26s

standards well before image net was
01:02:30.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m30s

around there was sci-fi 10 you might
01:02:33.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m33s

wonder why we're going to be looking at
01:02:35.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m35s

such an old data set and actually I
01:02:37.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m37s

think small data sets are much more
01:02:39.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m39s

interesting than image net because like
01:02:44.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m44s

most of the time you're likely to be
01:02:47.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m47s

working with stuff with a small number
01:02:48.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m48s

of thousands of images rather than one
01:02:51.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m51s

and a half million images some of you
01:02:54.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m54s

will work at one and a half million
01:02:56.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m56s

images but most of you won't right so
01:02:57.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m57s

learning how to use these kind of data
01:02:59.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h02m59s

sets I think is much more interesting
01:03:01.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m01s

often also a lot of the stuff we're
01:03:02.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m02s

looking at like in medical imaging we're
01:03:05.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m05s

looking at like the specific area where
01:03:07.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m07s

there's a lung nodule you're probably
01:03:09.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m09s

looking at like 32 by 32 pixels at most
01:03:11.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m11s

as being the area where that lung nodule
01:03:14.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m14s

actually exists right and so sci-fi 10
01:03:16.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m16s

is small both in terms of it doesn't
01:03:19.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m19s

have many images and the images are very
01:03:20.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m20s

small and so therefore I think this is
01:03:23.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m23s

like it's been in a lot of ways is much
01:03:25.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m25s

more challenging then something like
01:03:27.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m27s

image net and in some ways it's much
01:03:30.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m30s

more interesting right and also most
01:03:31.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m31s

importantly you can run stuff much more
01:03:34.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m34s

quickly on earth so it's much better to
01:03:36.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m36s

test out your algorithms with something
01:03:38.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m38s

you can run quickly and they're still
01:03:41.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m41s

challenging and so I hear a lot of
01:03:43.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m43s

researchers complain about like how they
01:03:45.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m45s

can't afford to study all the different
01:03:47.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m47s

versions that their algorithm properly
01:03:51.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m51s

because it's too expensive and they're
01:03:52.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m52s

doing that on imagenet so like it's
01:03:55.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m55s

literally a week of you know expensive
01:03:57.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m57s

CP GPU work for every study they do and
01:03:59.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h03m59s

like I don't understand why you would do
01:04:02.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m02s

that kind of study on imagenet doesn't
01:04:04.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m04s

make sense
01:04:06.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m06s

yeah and so
01:04:07.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m07s

this has been a particularly you know
01:04:10.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m10s

there's been a particular a lot of kind
01:04:13.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m13s

of debate about this this week because
01:04:15.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m15s

I'm really interesting researcher named
01:04:17.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m17s

Ali raha me nips this week gave a talk a
01:04:19.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m19s

really great talk about kind of the need
01:04:22.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m22s

for rigor in experiments in deep
01:04:24.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m24s

learning and you know he felt like
01:04:28.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m28s

there's a lack of rigor and I've talked
01:04:30.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m30s

to him about it quite a bit since that
01:04:31.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m31s

time and I'm not sure we yet quite
01:04:34.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m34s

understand each other as to where we're
01:04:38.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m38s

coming from but but we have very similar
01:04:40.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m40s

kinds of concerns which is basically
01:04:42.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m42s

people aren't doing carefully tuned
01:04:44.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m44s

carefully thought about experiments but
01:04:48.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m48s

instead they kind of throw lots of GPUs
01:04:50.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m50s

or lots of data and consider that a day
01:04:52.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m52s

and so this idea of like saying like
01:04:54.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m54s

well you know it's my data statement
01:04:57.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m57s

it's my algorithm meant to be good
01:04:59.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h04m59s

that's moral imagers at small data sets
01:05:01.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m01s

well if so let's study on sci-fi 10
01:05:04.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m04s

revin studying it on imagenet and then
01:05:07.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m07s

do more studies of different versions of
01:05:09.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m09s

the algorithm and learning different
01:05:12.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m12s

bits on and off understand which parts
01:05:13.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m13s

are actually important and so forth
01:05:15.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m15s

people also complain a lot about amnesty
01:05:18.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m18s

which we've talked about looked at
01:05:20.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m20s

before and I would say the same thing
01:05:23.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m23s

about em this right which is like if
01:05:25.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m25s

you're actually trying to understand
01:05:26.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m26s

rich parts of your algorithm make a
01:05:27.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m27s

difference and why using m mr that kind
01:05:29.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m29s

of study is a very good idea and all
01:05:32.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m32s

these people who complain about em NIST
01:05:34.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m34s

I think they're just showing off they're
01:05:36.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m36s

saying like oh I work at Google and I
01:05:38.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m38s

have you know a part of TP use and I
01:05:40.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m40s

have $100,000 a week of time just being
01:05:42.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m42s

on it no worries but I don't know I
01:05:44.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m44s

think that's all it is you know it's
01:05:48.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m48s

just signalling rather than actually
01:05:49.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m49s

academically rigorous okay so I'm so far
01:05:51.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m51s

ten you can download from here and this
01:05:55.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m55s

person is very kindly made it available
01:05:58.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m58s

in image form if you google for size 510
01:05:59.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h05m59s

you'll find us a much less convenient
01:06:04.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m04s

form so please use this one it's already
01:06:06.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m06s

in the exact form you need once you
01:06:09.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m09s

download it you can use it in the usual
01:06:10.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m10s

way so here's a list of the classes that
01:06:13.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m13s

are there now you'll see here I've
01:06:19.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m19s

created this thing called
01:06:22.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m22s

that's normally when we've been using
01:06:23.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m23s

pre trained models we have been seeing
01:06:26.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m26s

transforms from model and that's
01:06:31.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m31s

actually created the necessary
01:06:34.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m34s

transforms to convert our data set into
01:06:37.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m37s

a normalized data set based on the means
01:06:41.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m41s

and standard deviations of each channel
01:06:43.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m43s

in the original model that was trained
01:06:46.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m46s

in our case though this time we got a
01:06:48.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m48s

trainer model from scratch so we have no
01:06:50.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m50s

such thing so we actually need to tell
01:06:53.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m53s

it the mean and standard deviation of
01:06:55.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m55s

our data to normalize it okay and so in
01:06:59.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h06m59s

this case I haven't included the code
01:07:02.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m02s

here to do it you should try and try
01:07:04.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m04s

this yourself to confirm that you can do
01:07:06.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m06s

this and understand where it comes from
01:07:08.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m08s

but this is just the mean channel and
01:07:09.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m09s

the standard deviation per channel of
01:07:11.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m11s

all of the images alright so we're going
01:07:14.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m14s

to try and create a model from scratch
01:07:20.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m20s

and so the first thing we need is some
01:07:24.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m24s

transformations so for so far 10 people
01:07:26.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m26s

generally do data augmentation of simply
01:07:30.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m30s

flipping randomly horizontally so here's
01:07:34.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m34s

how we can create a specific list of
01:07:38.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m38s

augmentations to use and then they also
01:07:41.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m41s

tend to add a little bit of padding
01:07:44.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m44s

black padding around the edge and then
01:07:47.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m47s

randomly pick a 32 by 32 spot from
01:07:49.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m49s

within that pattern image so if you add
01:07:53.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m53s

the pad parameter to any of the fast a a
01:07:55.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m55s

transform creators it'll it'll do that
01:07:58.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h07m58s

for you okay and so in this case I'm
01:08:01.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m01s

just going to add 4 pixels around each
01:08:05.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m05s

size and so now that I've got my
01:08:09.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m09s

transforms I can go ahead and create my
01:08:12.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m12s

image classifier data from paths in the
01:08:14.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m14s

usual way okay I'm going to use a batch
01:08:18.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m18s

size of 256 because these are pretty
01:08:22.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m22s

small so it's going to let me do a
01:08:24.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m24s

little bit more at a time so here's what
01:08:26.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m26s

the data looks like so for example
01:08:28.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m28s

here's a boat and just to show you how
01:08:31.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m31s

tough this is what's that
01:08:33.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m33s

okay it is it's a not chicken prog-rock
01:08:37.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m37s

so I guess it's this big thing whatever
01:08:42.549
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m42s

the thing is called there's your frog
01:08:45.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m45s

okay so so these are the kinds of things
01:08:47.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m47s

that we want to look at so I'm going to
01:08:51.009
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m51s

start out so our student Karen we saw
01:08:54.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m54s

one of his posts earlier in this course
01:08:57.969
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h08m57s

he he made this really cool log book
01:09:00.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m00s

which shows how different optimizers
01:09:03.779
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m03s

works there we go so Karen made this
01:09:07.569
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m07s

really cool notebook I think it was
01:09:12.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m12s

maybe last week in which he showed how
01:09:14.109
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m14s

to create various different optimizers
01:09:16.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m16s

from scratch so this is kind of like the
01:09:19.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m19s

Excel thing I had but this is the Python
01:09:20.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m20s

version of momentum and Adam and
01:09:22.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m22s

Nesterov and Atta grad all written from
01:09:25.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m25s

scratch it is very cool one of the nice
01:09:27.279
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m27s

things he did was he showed a tiny
01:09:29.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m29s

little general-purpose fully connected
01:09:32.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m32s

Network generator so we're going to
01:09:35.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m35s

start with his so so he called that
01:09:37.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m37s

simple net so are we so here's a simple
01:09:39.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m39s

class which has a list of fully
01:09:42.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m42s

connected layers okay
01:09:47.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m47s

whenever you create a list of layers in
01:09:49.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m49s

pi torch you have to wrap it in an end
01:09:52.299
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m52s

module list just to tailpipe torch to
01:09:54.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m54s

like register these as attributes and so
01:09:57.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h09m57s

then we just go ahead and flatten the
01:10:01.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m01s

data that comes in because it's fully
01:10:04.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m04s

connected layers and then go through
01:10:05.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m05s

each layer and call that linear layer do
01:10:07.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m07s

the value to it and at the end do a soft
01:10:12.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m12s

mess okay so there's a really simple
01:10:15.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m15s

approach and so we can now take that
01:10:17.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m17s

model and now I'm going to show you how
01:10:21.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m21s

to step up one level of the API higher
01:10:23.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m23s

rather than calling the fit function
01:10:26.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m26s

we're going to create a learn object but
01:10:28.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m28s

we're going to create a learn object
01:10:30.429
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m30s

from a custom model and so we can do
01:10:31.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m31s

that by say we want a convolutional
01:10:35.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m35s

learner we want to create it from a
01:10:37.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m37s

model and from some data and the model
01:10:39.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m39s

is this one so this is just a general
01:10:42.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m42s

height watch model and this is a model
01:10:45.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m45s

data object of the usual kind
01:10:49.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m49s

and that will return a loaner so this is
01:10:51.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m51s

a bit easier than what we just saw with
01:10:53.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m53s

the RNN we don't have to fiddle around
01:10:55.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m55s

with lair optimizers and cosine and
01:10:57.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m57s

kneeling and callbacks and whatever this
01:10:59.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h10m59s

is now a loaner that we can do all the
01:11:01.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m01s

usual stuff with that we can do it with
01:11:03.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m03s

any model that we created okay so if we
01:11:06.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m06s

just go learn that'll go ahead and print
01:11:11.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m11s

it out okay so you can see we've got
01:11:14.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m14s

three thousand and seventy two features
01:11:16.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m16s

coming in because you've got 32 by 32
01:11:17.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m17s

pixels by three channels okay and then
01:11:19.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m19s

we've got 40 features coming out of the
01:11:22.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m22s

first layer that's going to go into the
01:11:24.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m24s

second layer ten features coming out
01:11:26.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m26s

because we've got the ten so far ten
01:11:28.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m28s

categories okay you can call dot summary
01:11:31.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m31s

to see that a little bit more detail we
01:11:35.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m35s

can do LR find we can plot that and we
01:11:38.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m38s

can then go fetch and we can use cycle
01:11:42.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m42s

length and so forth okay so with a
01:11:44.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m44s

simple
01:11:48.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m48s

how many hidden layers do we have one
01:11:50.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m50s

hidden layer right one getting layer one
01:11:52.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m52s

output layer one hidden layer model with
01:11:54.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m54s

and here we can see the number of
01:11:58.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h11m58s

parameters we have is that over 120,000
01:12:00.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m00s

okay we get a 47 percent accuracy so not
01:12:05.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m05s

great all right so let's kind of try and
01:12:13.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m13s

improve it right and so the goal here is
01:12:15.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m15s

we're going to try and eventually
01:12:18.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m18s

replicate the basic kind of architecture
01:12:19.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m19s

of a resonator okay so that's where
01:12:24.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m24s

we're going to try and get to hear it
01:12:26.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m26s

specially built up to a resonant so the
01:12:27.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m27s

first step is to replace our fully
01:12:30.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m30s

connected model with a convolutional
01:12:32.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m32s

model okay so to remind you so to remind
01:12:34.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m34s

you a fully connected layer is simply
01:12:44.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m44s

doing a dot product right so if we had
01:12:47.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m47s

like all of these data points and all of
01:12:50.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m50s

these weights right then we basically
01:12:54.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m54s

do a sum product of all of those
01:12:59.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h12m59s

together right in other words it's a
01:13:02.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m02s

matrix multiply right then that's a
01:13:03.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m03s

fully connected layer okay and so we
01:13:06.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m06s

need the white matrix is going to take
01:13:10.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m10s

contain an item for every every element
01:13:12.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m12s

of the input for every element of the
01:13:15.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m15s

output okay so that's why we have here a
01:13:17.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m17s

pretty big weight matrix and so that's
01:13:19.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m19s

why we had despite the fact that we have
01:13:26.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m26s

such a crappy accuracy we have a lot of
01:13:28.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m28s

parameters because in this very first
01:13:31.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m31s

layer
01:13:33.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m33s

we've got 3072 coming in and for T
01:13:34.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m34s

coming out so that gives us three
01:13:39.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m39s

thousand times forty parameters and so
01:13:41.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m41s

we end up not using them very
01:13:44.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m44s

efficiently because we're basically
01:13:46.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m46s

saying every single pixel in the input
01:13:48.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m48s

has a different weight and of course
01:13:50.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m50s

what we really want to do is kind of
01:13:52.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m52s

find groups of three by three pixels
01:13:53.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m53s

that have particular patterns to them
01:13:56.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m56s

okay and remember we call that a
01:13:58.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h13m58s

convolution okay so a convolution looks
01:14:00.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m00s

like so we have like little 3x3 section
01:14:10.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m10s

of our image and a corresponding 3x3 set
01:14:14.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m14s

of filters right or our filter with a
01:14:20.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m20s

three by three kernel and we just do a
01:14:22.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m22s

sum product of just that three by three
01:14:25.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m25s

by that three by three okay and then we
01:14:28.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m28s

do that for every single part of our
01:14:31.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m31s

image right and so when we do that
01:14:35.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m35s

across the whole image that's called a
01:14:37.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m37s

convolution and remember in this case we
01:14:38.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m38s

actually had multiple filters right so
01:14:42.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m42s

the result of that convolution actually
01:14:45.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m45s

had multiple it was a tensor with an
01:14:47.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m47s

additional third dimension to it
01:14:50.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m50s

effectively so let's take exactly the
01:14:53.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m53s

same code that we had before but we're
01:14:58.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h14m58s

going to replace n n dot linear with NN
01:15:01.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m01s

com2 D okay now what I want to do in
01:15:04.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m04s

this case though is each time I have a
01:15:09.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m09s

layer I want to
01:15:11.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m11s

the next layer smaller and so the way I
01:15:13.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m13s

did that in my Excel example was I used
01:15:17.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m17s

max Pauling right so max Pauling took
01:15:20.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m20s

every 2x2 section and replaced it with
01:15:24.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m24s

this maximum value right nowadays we
01:15:27.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m27s

don't use that kind of max bowling much
01:15:31.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m31s

at all instead nowadays what we tend to
01:15:34.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m34s

do is do what's called a stride to
01:15:37.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m37s

convolution
01:15:39.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m39s

let's drag to convolution rather than
01:15:40.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m40s

saying let's go through every single 3x3
01:15:43.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m43s

it says let's go through every second
01:15:49.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m49s

3x3 so rather than moving this three by
01:15:54.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m54s

three one to the right
01:15:57.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m57s

we move it two to the right and then
01:15:58.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h15m58s

when we get to the end of the row rather
01:16:01.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m01s

than moving one row down we move two
01:16:02.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m02s

rows down okay so that's called a stride
01:16:05.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m05s

to convolution and so it's tried to
01:16:08.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m08s

convolution has the same kind of effect
01:16:10.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m10s

as a max pooling which is you end up
01:16:13.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m13s

having the resolution in each dimension
01:16:15.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m15s

so we can ask for that by saying stroud
01:16:18.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m18s

equals to okay we can say we wanted to
01:16:21.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m21s

be three by three by saying kernel size
01:16:24.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m24s

and then the first term parameters are
01:16:26.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m26s

exactly the same as nn but linear
01:16:28.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m28s

they're the number of features coming in
01:16:30.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m30s

and the number of features coming out
01:16:31.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m31s

okay so we create a multiple list of
01:16:34.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m34s

those layers and then at the very end of
01:16:37.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m37s

that so in this case I'm going to say
01:16:42.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m42s

okay I've got three channels coming in
01:16:44.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m44s

the first one layer will come out with
01:16:46.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m46s

20 then a at 40 and then 80 so if we
01:16:48.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m48s

look at the summary we're going to start
01:16:52.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m52s

with a 32 by 32 we're going to spit out
01:16:53.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m53s

of 15 by 15 and then a 7 by 7 and then a
01:16:56.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h16m56s

3 by 3 right and so what do we do now to
01:17:01.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m01s

get that down to a prediction of one of
01:17:06.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m06s

10 classes what we do is we do something
01:17:08.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m08s

called adaptive max pooling and this is
01:17:12.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m12s

what is pretty standard now for
01:17:15.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m15s

state-of-the-art algorithms is that the
01:17:17.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m17s

very last layer we do a max pool but
01:17:19.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m19s

rather than doing like a 2
01:17:24.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m24s

go to next Paul we say like it doesn't
01:17:26.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m26s

have you to bow to could have been 3x3
01:17:30.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m30s

which is like replace every three by
01:17:31.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m31s

three pixels with its maximum could have
01:17:33.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m33s

been four by four adaptive backs Paul is
01:17:35.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m35s

where you say I'm not going to tell you
01:17:38.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m38s

how big an area to pull but instead I'm
01:17:41.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m41s

going to tell you how big a resolution
01:17:44.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m44s

to create right so if I said for example
01:17:46.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m46s

I think my input here is like 28 by 28
01:17:51.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m51s

right if I said do a 14 by 14 adaptive
01:17:54.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m54s

max Paul that would be the same as a 2
01:17:59.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h17m59s

by 2 max Paul because in other that's
01:18:01.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m01s

saying please create a 14 by 14 output
01:18:03.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m03s

if I said do a 2 by 2 adaptive max Paul
01:18:06.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m06s

right then that would be the same as
01:18:10.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m10s

saying do a 14 by 14 max Paul and so
01:18:12.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m12s

what we pretty much always do in modern
01:18:17.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m17s

cnn's is we make our per northmet layer
01:18:19.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m19s

a 1 by 1 adaptive Max Paul so in other
01:18:22.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m22s

words find the single largest cell and
01:18:28.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m28s

use that as our new activation right and
01:18:33.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m33s

so once we've got that we've now got a 1
01:18:39.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m39s

by 1 tensor right we're actually 1 by 1
01:18:43.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m43s

by number of features tensor so we can
01:18:47.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m47s

then on top of that go view X dot view X
01:18:50.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m50s

dot size comma minus 1 and actually
01:18:54.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m54s

there are no other dimensions to this
01:18:57.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h18m57s

basically right so this is going to
01:19:01.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m01s

return a matrix of mini-batch by number
01:19:03.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m03s

of features and so then we can feed that
01:19:07.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m07s

into a linear layer with however many
01:19:10.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m10s

classes we need right so you can see
01:19:15.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m15s

here the last thing I pass in is how
01:19:18.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m18s

many classes am I trying to predict and
01:19:20.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m20s

that's what's going to be used to create
01:19:22.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m22s

that last layer so it goes through every
01:19:23.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m23s

convolutional layer does a convolution
01:19:26.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m26s

does arel you does an adaptive max pool
01:19:28.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m28s

this dot view just gets rid of those
01:19:33.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m33s

trailing unit backsies
01:19:36.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m36s

one comma one axis which is not
01:19:39.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m39s

necessary that allows us to fit that
01:19:42.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m42s

into our final linear layer that spits
01:19:45.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m45s

out something of size C which here is
01:19:47.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m47s

ten so you can now see how it works
01:19:51.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m51s

it goes 32 to 15 to 7 by 7 to 3 by 3 the
01:19:54.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m54s

adaptive next pull makes it 80 by 1 by 1
01:19:59.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h19m59s

right and then our dot view makes it
01:20:04.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m04s

just a mini batch size by 80 and then
01:20:07.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m07s

finally a linear layer which makes it
01:20:10.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m10s

from 80 to 10 which is what we wanted
01:20:12.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m12s

okay so that's our like most basic you
01:20:15.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m15s

would call this a fully convolutional
01:20:19.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m19s

network so a fully convolutional network
01:20:21.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m21s

is something where every layer is
01:20:23.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m23s

convolutional except for the very last
01:20:25.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m25s

so again we can now go Li dot find and
01:20:31.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m31s

now in this case when I did ll find it
01:20:35.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m35s

went through the entire data set and
01:20:39.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m39s

we're still getting better and so in
01:20:41.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m41s

other words even a the default final
01:20:43.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m43s

learning rate rises 10 and even at that
01:20:46.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m46s

point it was still like pretty much
01:20:48.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m48s

getting better so you can always
01:20:50.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m50s

override the final learning rate by
01:20:52.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m52s

saying n del R equals that and that will
01:20:54.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m54s

get it just to get it to try morphine's
01:20:57.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m57s

okay and so here is the learning rate
01:20:58.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h20m58s

finder and so I picked 10 to the minus 1
01:21:01.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m01s

trained that for a while and that's
01:21:06.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m06s

looking pretty good
01:21:09.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m09s

so I try to put the cycle length of 1
01:21:10.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m10s

and it's starting to flatten out at
01:21:12.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m12s

about 60% right so you can see here the
01:21:14.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m14s

number of elements the number of
01:21:18.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m18s

parameters I have here are 500 7000
01:21:20.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m20s

28,000 about 30,000 right so I have
01:21:24.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m24s

about a quarter of the number of routers
01:21:28.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m28s

that my accuracy has gone up from 47% to
01:21:30.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m30s

60% right and the time per epoch here is
01:21:34.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m34s

under 30 seconds and here also so the
01:21:38.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m38s

time period box about the same and
01:21:43.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m43s

that's not surprising because when you
01:21:44.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m44s

use small simple architectures most of
01:21:46.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m46s

the time is the memory transfer the
01:21:49.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m49s

actual time during the compute is
01:21:51.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m51s

is trivial okay so I'm going to refactor
01:21:53.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m53s

this slightly because I want to try and
01:21:58.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h21m58s

put less stuff inside my forward and so
01:22:01.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m01s

calling RAL you every time you know it
01:22:05.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m05s

doesn't seem ideal so I'm going to
01:22:07.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m07s

create a new class called conf lair okay
01:22:09.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m09s

and the conf lair class is going to
01:22:13.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m13s

contain a convolution with a kernel size
01:22:15.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m15s

of three and a stride of two one thing
01:22:18.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m18s

I'm going to do now is I'm going to add
01:22:21.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m21s

padding
01:22:22.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m22s

did you notice here the first layer went
01:22:22.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m22s

from 32 by 32 to 15 by 15 not 16 by 16
01:22:25.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m25s

and the reason for that is that at the
01:22:30.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m30s

very edge of your convolution right here
01:22:33.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m33s

see how this first convolution like
01:22:42.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m42s

there isn't a convolution where the
01:22:45.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m45s

middle is the top left point right
01:22:46.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m46s

because there's like nothing outside it
01:22:49.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m49s

where else if we had put a row of zeros
01:22:52.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m52s

at the top and a row of zeros at the
01:22:55.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m55s

edge of each column we now could go all
01:22:57.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h22m57s

the way to the edge alright so pad
01:23:01.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m01s

equals 1 adds that little layer of zeros
01:23:03.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m03s

around the edge for us ok
01:23:09.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m09s

and so this way we're going to make sure
01:23:10.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m10s

that we go 32 by 32 to 16 by 16 to 8 by
01:23:12.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m12s

8 it doesn't matter too much when you've
01:23:16.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m16s

got these bigger layers but by the time
01:23:18.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m18s

you get down to like say 4 by 4 you
01:23:20.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m20s

really don't want to throw away a whole
01:23:23.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m23s

piece right so padding becomes important
01:23:25.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m25s

so by refactoring it to put this with
01:23:27.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m27s

its defaults here and then in the
01:23:32.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m32s

forward I put the value in here as well
01:23:33.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m33s

it makes by confident you know a little
01:23:35.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m35s

bit smaller and you know more to the
01:23:39.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m39s

point it's going to be easier for me to
01:23:41.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m41s

make sure that everything is correct in
01:23:42.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m42s

the future by always using this common
01:23:44.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m44s

player class ok
01:23:46.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m46s

so now you know not only how to create
01:23:47.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m47s

your own neural network model but how to
01:23:50.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m50s

create your own neural network layer so
01:23:52.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m52s

here now I can use conf layer right and
01:23:55.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m55s

this is such a cool thing about pi torch
01:23:58.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h23m58s

is a layer definition and a neural
01:24:00.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m00s

network definition are literally
01:24:03.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m03s

identical okay they both
01:24:04.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m04s

have a constructor and a forward and so
01:24:06.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m06s

anytime you've got the lair you can use
01:24:09.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m09s

it as a neural net anytime you have a
01:24:12.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m12s

neural net you can use it as a lair
01:24:13.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m13s

okay so this is now the exact same thing
01:24:15.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m15s

as we had before one difference is I now
01:24:18.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m18s

have padding okay and another thing just
01:24:21.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m21s

to show you you can do things
01:24:23.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m23s

differently back here my max pool I did
01:24:24.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m24s

as as an object likely I use the class n
01:24:29.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m29s

n dot adaptive max pool and I stuck it
01:24:33.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m33s

in this attribute and then I called it
01:24:35.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m35s

but this actually doesn't have any state
01:24:37.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m37s

there's no weights inside max pooling so
01:24:40.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m40s

I can actually do it with a little bit
01:24:44.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m44s

less code by calling it as a function
01:24:45.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m45s

right so everything that you can do as a
01:24:47.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m47s

class you can also do as a function it's
01:24:50.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m50s

inside this capital F which is n n dot
01:24:52.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m52s

functional okay so this should be a tiny
01:24:54.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m54s

bit better because this time I've got
01:24:59.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h24m59s

the padding I didn't trade it for as
01:25:03.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m03s

long to actually check so let's skip
01:25:06.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m06s

over that all right so one issue here is
01:25:09.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m09s

that in the end this is having I when I
01:25:14.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m14s

tried to add more layers
01:25:19.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m19s

I had travel training it okay and the
01:25:21.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m21s

reason I was having trouble training it
01:25:26.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m26s

is it was you know if I used larger
01:25:27.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m27s

learning rates it would go off to NI N
01:25:30.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m30s

and if I use smaller learning rates that
01:25:31.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m31s

are kind of takes forever and doesn't
01:25:34.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m34s

really have a chance to explore properly
01:25:35.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m35s

so it wasn't resilient so to make my
01:25:37.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m37s

model more resilient I'm going to use
01:25:41.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m41s

something called batch normalization
01:25:43.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m43s

which literally everybody calls bachelor
01:25:44.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m44s

and bachelorettes a couple of years old
01:25:47.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m47s

now and it's been pretty transformative
01:25:51.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m51s

since it came along because it suddenly
01:25:54.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m54s

makes it really easy to train deeper
01:25:57.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h25m57s

networks alright so the network I'm
01:26:00.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m00s

going to create is going to have more
01:26:02.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m02s

layers right I've got one two three four
01:26:04.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m04s

five convolutional layers plus a fully
01:26:07.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m07s

connected layer right so like back in
01:26:09.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m09s

the old days that would be considered a
01:26:11.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m11s

pretty deep network and we considered
01:26:13.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m13s

pretty hard to train nowadays super
01:26:15.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m15s

simple thanks to vaginal
01:26:17.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m17s

now to use batch norm you can just write
01:26:20.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m20s

in end on that to learn about it we're
01:26:23.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m23s

going to write it from scratch okay so
01:26:25.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m25s

the basic idea of batch norm is that
01:26:28.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m28s

we've got some vector of activations
01:26:32.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m32s

anytime I draw a vector of activations
01:26:35.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m35s

obviously I mean you can repeat it for
01:26:37.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m37s

the mini batch so I pretend it's a mini
01:26:39.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m39s

batch with one so we've got some veteran
01:26:41.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m41s

activations and it's coming into some
01:26:43.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m43s

layer right so so probably some
01:26:46.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m46s

convolutional matrix multiplication and
01:26:49.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m49s

then something comes out the other side
01:26:52.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m52s

so imagine this this is just a matrix
01:26:55.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m55s

multiply which was like I don't know say
01:26:58.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h26m58s

it was a identity matrix right then
01:27:01.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m01s

every time I'd multiply it by that
01:27:11.200
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m11s

across lots and lots of layers my
01:27:12.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m12s

activations are not getting bigger
01:27:14.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m14s

they're not getting smaller they're not
01:27:16.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m16s

changing at all okay that's all fine
01:27:17.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m17s

right but imagine if it was actually
01:27:20.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m20s

like 2 2 2 right and so if every one of
01:27:22.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m22s

my weight matrices or filters was like
01:27:28.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m28s

that then my activations are doubling
01:27:29.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m29s

each time right and so suddenly I've got
01:27:32.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m32s

as exponential growth and that in deep
01:27:35.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m35s

models that's going to be a disaster
01:27:40.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m40s

right because my gradients are exploding
01:27:41.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m41s

at an exponential rate and so the
01:27:44.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m44s

challenge you have is that it's it's
01:27:47.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m47s

very unlikely unless you try carefully
01:27:50.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m50s

to deal with it that your matrices your
01:27:54.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m54s

weight matrices on average are not going
01:27:57.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h27m57s

to cause your activations to keep
01:28:00.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m00s

getting smaller and smaller or keep
01:28:03.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m03s

getting bigger and bigger right you have
01:28:04.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m04s

to kind of carefully control things to
01:28:06.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m06s

make sure that they stay you know at a
01:28:08.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m08s

reasonable size you want to you know
01:28:11.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m11s

keep them at a reasonable scale so we
01:28:13.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m13s

start things off with zero mean standard
01:28:16.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m16s

deviation one by normalizing the inputs
01:28:20.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m20s

really like to do is to normalize every
01:28:23.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m23s

layer not just the inputs all right and
01:28:27.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m27s

so okay fine
01:28:30.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m30s

let's do that right so here I've created
01:28:34.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m34s

a BN layer which is exactly like my
01:28:37.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m37s

Kampf layer it's got my common 2d with
01:28:39.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m39s

my stride my padding right I do my
01:28:42.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m42s

condom I value right and then I
01:28:45.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m45s

calculate the mean of each channel or of
01:28:48.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m48s

each filter and the standard deviation
01:28:53.429
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m53s

of each channel or each filter and then
01:28:56.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m56s

I subtract the means and divide by the
01:28:58.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h28m58s

standard deviations right so now I don't
01:29:02.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m02s

actually need to normalize my input at
01:29:06.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m06s

all because it's actually going to do it
01:29:09.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m09s

automatically
01:29:11.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m11s

right it's normalizing it per channel or
01:29:11.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m11s

and for later layers its normalizing it
01:29:15.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m15s

per filter so it turns out that's not
01:29:17.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m17s

enough right
01:29:22.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m22s

because SGD is bloody-minded right and
01:29:24.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m24s

so if sgt decided that it what's the
01:29:29.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m29s

weight matrix to be you know like so
01:29:33.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m33s

where that matrix is something which is
01:29:35.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m35s

going to you know increase the values
01:29:38.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m38s

overall repeatedly then trying to divide
01:29:41.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m41s

it by the subtract domains and divide by
01:29:45.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m45s

the standard deviations just means the
01:29:47.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m47s

next mini-batch it's going to try and do
01:29:49.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m49s

it again and they were try and do it
01:29:51.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m51s

again it'll try and do it again so it
01:29:52.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m52s

turns out that this actually doesn't
01:29:54.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m54s

help like it literally does nothing
01:29:57.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m57s

because SGD is just going to go ahead
01:29:58.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h29m58s

and undo it
01:30:01.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m01s

the next mini batch so what we do is we
01:30:03.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m03s

create a new multiplier for each channel
01:30:08.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m08s

and a new added value for each Channel
01:30:13.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m13s

literally just and we just start them
01:30:19.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m19s

out as the addition and addition is just
01:30:21.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m21s

a bunch of zeros so for the first layer
01:30:23.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m23s

three zeros and the multiplier for the
01:30:25.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m25s

first layer is just three ones okay so
01:30:28.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m28s

number of filters for the first layer is
01:30:31.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m31s

just three and so we then like basically
01:30:33.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m33s

undo exactly what we just did or
01:30:36.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m36s

potentially we undo them right so by
01:30:39.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m39s

saying this is an addenda parameter that
01:30:42.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m42s

tells PI torch you're allowed
01:30:44.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m44s

to learn these as weights right so
01:30:47.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m47s

initially it says okay so check the
01:30:50.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m50s

means divided by the standard deviations
01:30:52.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m52s

multiplied by one add on zero
01:30:54.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m54s

okay that's fine nothing much happened
01:30:59.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h30m59s

there but what it turns out is that now
01:31:01.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m01s

rather than like if it wants to kind of
01:31:05.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m05s

scale the layer up it doesn't have to
01:31:08.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m08s

scale up every single value in the
01:31:11.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m11s

matrix it can just scale up this single
01:31:13.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m13s

trio of numbers self dot M if it wants
01:31:17.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m17s

to shift it all Apple down a bit doesn't
01:31:21.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m21s

have to shift the entire weight matrix
01:31:23.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m23s

they can just shift this trio of numbers
01:31:26.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m26s

self dot a so I will say this I'm at
01:31:28.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m28s

this talk I mentioned at nips Alley
01:31:34.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m34s

Rahim ease talk about rigor he actually
01:31:36.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m36s

pointed to this paper this batch norm
01:31:38.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m38s

paper as being a particularly useful
01:31:40.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m40s

particularly interesting paper where a
01:31:44.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m44s

lot of people don't necessarily we quite
01:31:48.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m48s

quite know why it was right and so if
01:31:52.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m52s

you're thinking like okay subtracting
01:31:56.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m56s

out the means and then adding some
01:31:58.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h31m58s

learned weights of exactly the same rank
01:32:00.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m00s

and size sounds like a weird thing to do
01:32:05.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m05s

there are a lot of people that feel the
01:32:09.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m09s

same way right so at the moment I think
01:32:11.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m11s

the best is I can say like intuitively
01:32:15.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m15s

is what's going on here is that we're
01:32:17.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m17s

normalizing the data and then we're
01:32:21.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m21s

saying you can then shift it and scale
01:32:24.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m24s

it using far fewer parameters than would
01:32:28.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m28s

have been necessary if I was asking you
01:32:31.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m31s

to actually shift and scale the entire
01:32:34.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m34s

set of convolutional filters right
01:32:36.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m36s

that's the kind of basic intuition more
01:32:39.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m39s

importantly in practice what this does
01:32:42.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m42s

is it adds is it basically allows us to
01:32:46.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m46s

increase our learning rates and it
01:32:50.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m50s

increases the resilience of training and
01:32:52.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m52s

allows us to add more layers so once I
01:32:54.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m54s

added
01:32:58.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h32m58s

a PN layer rather than a common flower I
01:33:00.139
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m00s

found I was able to add more layers to
01:33:05.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m05s

my model and it still trained
01:33:08.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m08s

effectively generally are we worried
01:33:10.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m10s

about anything that maybe we are divided
01:33:17.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m17s

by something very small or anything like
01:33:20.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m20s

that once we do this probably I think in
01:33:24.619
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m24s

the pie chart
01:33:28.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m28s

version it would probably be divided by
01:33:29.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m29s

itself dudes plus Epsilon or something
01:33:33.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m33s

yeah this worked fine for me but yeah
01:33:35.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m35s

that is definitely something to think
01:33:42.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m42s

about if you were trying to make this
01:33:43.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m43s

more reliable I mentioned poplar so the
01:33:45.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m45s

self dot m and self dot a getting it's
01:33:52.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m52s

getting updated through back propagation
01:33:55.639
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m55s

as well yeah so by putting like saying
01:33:57.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h33m57s

it's an N n dot parameter that's how we
01:34:00.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m00s

flag to pi torch to learn it through
01:34:02.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m02s

that probe exactly right the other
01:34:05.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m05s

interesting thing it turns out the batch
01:34:10.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m10s

norm does is it regularizes in other
01:34:11.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m11s

words you can often decrease or remove
01:34:16.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m16s

drop out or decrease or remove weight
01:34:18.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m18s

okay when you use batch normal and the
01:34:21.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m21s

reason why is if you think about it each
01:34:23.869
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m23s

mini batch is going to have a different
01:34:27.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m27s

mean and a different standard deviation
01:34:30.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m30s

to the previous mini batch so these
01:34:32.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m32s

things keep changing and because they
01:34:34.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m34s

keep changing it's kind of changing the
01:34:37.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m37s

meaning of the filters in this subtle
01:34:39.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m39s

way and so it's adding a regularization
01:34:41.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m41s

effect because it's noise that when you
01:34:44.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m44s

add noise of any kind it regularizes
01:34:46.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m46s

your model all right I'm actually
01:34:50.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m50s

cheating a little bit here in the real
01:34:52.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m52s

version of batch norm you don't just use
01:34:55.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m55s

this batches mean and standard deviation
01:34:59.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h34m59s

but instead you take an exponentially
01:35:01.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m01s

weighted moving average standard
01:35:04.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m04s

deviation and
01:35:07.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m07s

and so if you wanted to exercise to try
01:35:08.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m08s

a during the week that would be a good
01:35:10.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m10s

thing to try but I will point out
01:35:12.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m12s

something very important here which is
01:35:15.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m15s

if self-training when we are doing our
01:35:16.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m16s

training loop this will be true when
01:35:23.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m23s

it's being applied to the training set
01:35:25.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m25s

and it will be false when it's being
01:35:28.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m28s

applied to the validation set and this
01:35:30.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m30s

is really important because when you're
01:35:33.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m33s

going through the validation set you do
01:35:35.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m35s

not want to be changing the meaning of
01:35:36.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m36s

the model okay so this is this really
01:35:38.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m38s

important idea is that there are some
01:35:42.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m42s

types of layer that are actually
01:35:45.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m45s

sensitive to what the mode of the of the
01:35:48.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m48s

network is whether it's in training mode
01:35:53.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m53s

or as plight which calls it evaluation
01:35:55.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m55s

mode or we might say a test mode right
01:35:58.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h35m58s

and actually we actually had a bug a
01:36:00.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m00s

couple of weeks ago when we did our mini
01:36:03.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m03s

net for movie lens the collaborative
01:36:06.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m06s

filtering we actually had F dot dropout
01:36:08.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m08s

in our forward pass without protecting
01:36:11.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m11s

it with a F self training F dot dropout
01:36:14.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m14s

as a result of which we were actually
01:36:18.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m18s

doing dropout in the validation piece as
01:36:20.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m20s

well as the training piece which
01:36:23.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m23s

obviously isn't what you want okay
01:36:25.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m25s

so I've actually gone back and fixed
01:36:26.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m26s

this by changing it to using n n dot
01:36:28.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m28s

dropout and n n dot dropout has already
01:36:32.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m32s

been written for us to check whether
01:36:36.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m36s

it's being used in training mode or not
01:36:38.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m38s

that or alternatively I could have added
01:36:40.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m40s

a if self dot training before I use the
01:36:43.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m43s

dropout yeah okay so it's important to
01:36:47.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m47s

think about that you know any and the
01:36:50.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m50s

main the main true or pretty much the
01:36:53.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m53s

only two built-in two pi torch where
01:36:54.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m54s

this happens is dropout and and so
01:36:57.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h36m57s

interestingly this is also a key
01:37:02.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m02s

difference in fast AI which no other
01:37:05.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m05s

library does is that these means and
01:37:07.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m07s

standard deviations get updated in
01:37:12.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m12s

training mode in every other library as
01:37:16.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m16s

soon as you basically say I'm
01:37:20.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m20s

training regardless even of whether that
01:37:22.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m22s

layer is set to trainable or not and it
01:37:24.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m24s

turns out that with a pre trained
01:37:27.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m27s

network that's a terrible idea
01:37:28.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m28s

if you have a pre trained network for
01:37:30.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m30s

specific values of those means and
01:37:33.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m33s

standard deviations in batch norm if you
01:37:35.139
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m35s

change them it changes the meaning of
01:37:37.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m37s

those pre trained layers right and so in
01:37:39.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m39s

fast AI always by default it won't touch
01:37:42.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m42s

those means and standard deviations if
01:37:45.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m45s

your layer is frozen okay as soon as you
01:37:47.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m47s

I'm freezing it'll start updating them
01:37:51.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m51s

unless you've set won't be and freeze
01:37:54.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m54s

true if you set learned up being freeze
01:37:59.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h37m59s

true it says never touch these met means
01:38:02.559
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m02s

and standard deviations and you know
01:38:05.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m05s

I've found in practice that that often
01:38:07.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m07s

seems to work a lot better for
01:38:12.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m12s

pre-trained models particularly if
01:38:13.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m13s

you're working with data that's quite
01:38:16.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m16s

similar to what the pre trained model
01:38:17.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m17s

was trained with you know as you look
01:38:19.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m19s

like I did a lot of work did you say
01:38:33.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m33s

sorry like quite a lot of code here well
01:38:35.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m35s

you're doing more work than you would
01:38:38.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m38s

normally do essentially you're
01:38:40.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m40s

calculating all these aggregates as you
01:38:42.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m42s

go through each each each layer yes
01:38:44.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m44s

wouldn't this mean you're training like
01:38:46.559
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m46s

your epoch time like now this is like
01:38:49.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m49s

super fast like if you think about what
01:38:52.719
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m52s

a cone has to do a cones has to go
01:38:54.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m54s

through every 3x3 you know with a stride
01:38:58.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h38m58s

and do this multiplication and then
01:39:01.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m01s

addition like that is a lot more work
01:39:03.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m03s

than simply calculating the per channel
01:39:06.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m06s

mean so this is so and that's a little
01:39:10.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m10s

bit of time but it's it's less time
01:39:12.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m12s

intensive than the convolution would it
01:39:14.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m14s

be like right after like decomposition
01:39:21.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m21s

yeah
01:39:25.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m25s

we'll talk about that in a moment so at
01:39:26.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m26s

the moment we have it after the rally
01:39:28.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m28s

and in the original batch norm paper I
01:39:32.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m32s

will
01:39:34.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m34s

that's where they put it so this this
01:39:35.209
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m35s

idea of something called an ablation
01:39:41.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m41s

study and an ablation study is something
01:39:43.939
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m43s

where you basically try kind of turning
01:39:48.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m48s

on and off different pieces of your
01:39:52.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m52s

model to see like which bits make which
01:39:55.729
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m55s

impacts and one of the things that
01:39:57.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m57s

wasn't done in the original batch norm
01:39:59.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h39m59s

paper was any kind of really effective
01:40:01.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m01s

ablation study and one of the things
01:40:04.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m04s

therefore that was missing was this
01:40:06.979
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m06s

question which you just asked which is
01:40:08.209
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m08s

like where do you put the vaginal before
01:40:10.039
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m10s

the early year after the earlier
01:40:12.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m12s

whatever and so since that time you know
01:40:13.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m13s

that oversight has caused a lot of
01:40:16.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m16s

problems because it turned out the
01:40:18.979
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m18s

original paper didn't actually put it in
01:40:20.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m20s

the best spot and so then other people
01:40:22.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m22s

since then have now figured that out
01:40:25.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m25s

and they're like every time I show
01:40:27.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m27s

people code where it's actually in the
01:40:29.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m29s

spot that turns out to be better people
01:40:31.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m31s

always say your bedrooms in the wrong
01:40:33.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m33s

spot and I have to go back and say no I
01:40:35.539
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m35s

know that's what the paper said what
01:40:37.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m37s

they're doing now that's what I thought
01:40:38.539
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m38s

and so it's kind of causes confusion so
01:40:39.889
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m39s

there's there's been a lot of question
01:40:42.709
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m42s

about that
01:40:44.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m44s

so a little bit of a higher-level
01:40:46.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m46s

question so we started out with cipher
01:40:49.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m49s

data yes it's the basic reasoning that
01:40:53.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m53s

you use a smaller data set to quickly
01:40:59.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h40m59s

train a new model and then you take it
01:41:02.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m02s

the same model and you're using much
01:41:07.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m07s

much much bigger data set to get a
01:41:11.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m11s

higher accuracy level is that the basic
01:41:14.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m14s

maybe so if you want to you know if you
01:41:17.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m17s

had a large data set or if you were like
01:41:21.079
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m21s

interested in the question of like how
01:41:23.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m23s

good is this technique on a large data
01:41:27.709
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m27s

set then yes what you just said would be
01:41:29.539
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m29s

what I would do I would do lots of
01:41:31.969
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m31s

testing on a small data set which I had
01:41:33.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m33s

already discovered had the same kinds of
01:41:36.499
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m36s

properties as my larger data set and
01:41:39.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m39s

therefore my conclusions would likely
01:41:41.539
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m41s

carry forward and then our test them at
01:41:43.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m43s

the end
01:41:45.229
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m45s

having said that personally I matched
01:41:45.559
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m45s

be more interested in actually studying
01:41:48.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m48s

small datasets for their own sake
01:41:53.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m53s

because I find most people I speak to in
01:41:55.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m55s

the real world don't have a million
01:41:59.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h41m59s

images they have you know somewhere
01:42:01.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m01s

between about two thousand and twenty
01:42:03.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m03s

thousand images seems to be much more
01:42:05.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m05s

common so I'm very you know very
01:42:06.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m06s

interested in having fewer rows because
01:42:10.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m10s

I think it's more valuable in factors
01:42:14.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m14s

I'm also pretty interested in small
01:42:16.930
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m16s

images not just for the rest you
01:42:19.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m19s

mentioned which is it allows me to test
01:42:21.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m21s

things out more quickly but also as I
01:42:24.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m24s

mentioned before often a small part of
01:42:26.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m26s

an image actually turns out to be what
01:42:29.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m29s

you're interested in that's certainly
01:42:30.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m30s

true in in medicine I have two questions
01:42:32.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m32s

the first is on what you mentioned in
01:42:38.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m38s

terms of small datasets particular
01:42:41.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m41s

middle medical imaging you've you've
01:42:43.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m43s

heard of I guess is it vicarious to
01:42:44.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m44s

start up in the specialization and
01:42:47.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m47s

one-shot learning so your opinions on
01:42:48.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m48s

that and then this second being this is
01:42:50.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m50s

related to I guess Ali's talk at nips so
01:42:53.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m53s

it was I don't say its controversial but
01:42:57.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m57s

like young laocon there was like a
01:42:58.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h42m58s

really
01:43:01.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m01s

I guess controversial thread attacking
01:43:01.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m01s

you in terms of what you're talking
01:43:04.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m04s

about as a baseline of theory just not
01:43:05.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m05s

keeping up with practice and so I mean I
01:43:08.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m08s

guess I was siding with the on where's
01:43:11.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m11s

all he actually he tweeted at me quite a
01:43:13.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m13s

bit trying to defend like he wasn't
01:43:15.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m15s

attacking yawn at all but in fact he was
01:43:17.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m17s

you know trying to support him but I
01:43:22.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m22s

just kind of feel like a lot of theory
01:43:24.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m24s

as as you go is just sort of at it they
01:43:26.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m26s

even it's hard to keep up whether then
01:43:29.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m29s

you know no archive from on Draco party
01:43:31.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m31s

to keep up but if the theory isn't
01:43:33.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m33s

keeping up but industry is the one
01:43:35.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m35s

that's actually sitting in the standard
01:43:37.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m37s

then doesn't that mean that you know
01:43:38.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m38s

people who are actual practitioners are
01:43:40.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m40s

the ones like young lacunae are
01:43:43.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m43s

publishing the theory that are keeping
01:43:44.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m44s

up to date or is like academic research
01:43:45.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m45s

institutions are actually behind so I
01:43:47.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m47s

don't have any comments on the vicarious
01:43:49.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m49s

papers because I haven't read them I'm
01:43:50.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m50s

not aware of any of them have as
01:43:52.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m52s

actually showing you know better results
01:43:54.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m54s

than the papers but I think they've come
01:43:59.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h43m59s

a long way in the last twelve
01:44:01.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m01s

so that might be wrong yeah yeah I
01:44:02.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m02s

viewed the discussion between yarn
01:44:05.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m05s

lacunae and a lyric Jimmy is very
01:44:06.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m06s

interesting because they're both smart
01:44:08.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m08s

people who have interesting things to
01:44:09.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m09s

say unfortunately a lot of people talk
01:44:11.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m11s

Ally's talk as meaning something which
01:44:16.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m16s

he says it didn't mean and when I
01:44:19.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m19s

listened to his talk I'm not sure he
01:44:21.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m21s

didn't actually made it at the time but
01:44:23.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m23s

he clearly doesn't mean it now which is
01:44:25.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m25s

he's he's now said many times he didn't
01:44:27.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m27s

he was not talking about theory he was
01:44:29.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m29s

not saying we need more theory at all
01:44:31.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m31s

actually he thinks we need more
01:44:33.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m33s

experiments and so specifically he's
01:44:35.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m35s

he's also now saying he wished he hadn't
01:44:39.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m39s

used the word rigour which I also wish
01:44:41.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m41s

because rigour is it's kind of
01:44:43.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m43s

meaningless and everybody can kind of
01:44:46.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m46s

say when he says rigor he means the
01:44:48.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m48s

specific thing I study you know so a
01:44:50.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m50s

lots of people have kind of taken his
01:44:55.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m55s

talk as being like oh yes this proves
01:44:57.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m57s

that nobody else should work in neural
01:44:59.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h44m59s

networks unless they are experts at the
01:45:01.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m01s

one thing I'm an expert in so yeah so
01:45:04.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m04s

I'm going to catch up with him and talk
01:45:08.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m08s

about more about this in January and
01:45:09.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m09s

hopefully we'll pick up some more stuff
01:45:11.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m11s

out together but basically what would we
01:45:12.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m12s

can clearly agree on and I think you and
01:45:16.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m16s

also agrees on is careful experiments
01:45:18.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m18s

are important just doing things on
01:45:22.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m22s

massive amounts of data using massive
01:45:25.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m25s

amounts of TP use or GP users not
01:45:27.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m27s

interesting of itself and we should
01:45:30.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m30s

instead try to design experiments that
01:45:32.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m32s

give us the maximum amount of insight
01:45:35.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m35s

into what's going on so Jeremy is it a
01:45:36.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m36s

good statement to say something like so
01:45:42.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m42s

drop out and bash norm are very
01:45:46.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m46s

different things drop out is the
01:45:51.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m51s

realization technique bash norm has
01:45:54.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m54s

maybe some realization effect but it's
01:45:57.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m57s

actually just about convergence of the
01:45:59.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h45m59s

optimization method yeah yeah and and I
01:46:02.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m02s

would further say like I can't see any
01:46:06.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m06s

reason not to use pattern or
01:46:10.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m10s

there are versions of batch norm that in
01:46:12.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m12s

certain situations turned out not to
01:46:16.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m16s

work so well that people have figured
01:46:18.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m18s

out ways around that for nearly every
01:46:22.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m22s

one of those situations now so I would
01:46:24.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m24s

always seek to find a way to use batch
01:46:26.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m26s

norm it may be a little harder in our
01:46:29.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m29s

own ends
01:46:32.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m32s

at least but even there there are ways
01:46:33.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m33s

of doing batch norm in our attenders as
01:46:37.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m37s

well so you know try try and always use
01:46:39.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m39s

batch norm on every layer if you can and
01:46:41.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m41s

the question that somebody asked is does
01:46:44.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m44s

it mean I have to I can stop normalize
01:46:47.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m47s

in my data yeah yeah it does
01:46:51.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m51s

although do it anyway because it's not
01:46:57.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h46m57s

at all hard to do it and at least that
01:47:03.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m03s

way the people using your data I don't
01:47:06.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m06s

know they kind of know how you've
01:47:09.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m09s

normalized it and particularly with
01:47:11.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m11s

these issues around a lot of libraries
01:47:15.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m15s

in my opinion at least warm not my
01:47:17.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m17s

opinion my experiments don't deal with
01:47:19.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m19s

batch norm correctly for pre-trained
01:47:23.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m23s

models just remember that when somebody
01:47:25.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m25s

starts retraining those averages and
01:47:27.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m27s

stuff are going to change for your data
01:47:31.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m31s

set and so if your new data set has very
01:47:32.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m32s

different input averages it could really
01:47:35.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m35s

cause a lot of problems so so yeah I
01:47:36.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m36s

went through a period where I actually
01:47:40.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m40s

stopped normalizing my data and you know
01:47:43.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m43s

things kind of worked but it's probably
01:47:46.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m46s

not worth it okay so so the rest of this
01:47:49.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m49s

is identical right all I've done is I've
01:47:55.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m55s

changed conf layer to BN layer but I've
01:47:58.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h47m58s

done one more thing which is I'm kind of
01:48:02.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m02s

trying to get closer and closer to
01:48:04.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m04s

modern approaches which I've added a
01:48:06.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m06s

single convolutional layer at the start
01:48:08.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m08s

with a bigger kernel size and a stride
01:48:11.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m11s

of one why have I done that so the basic
01:48:14.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m14s

idea is that I want my first layer to
01:48:19.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m19s

kind of have a richer input right so
01:48:23.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m23s

before my first layer had an input of
01:48:26.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m26s

just three because there's just three
01:48:28.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m28s

channels right but if I start with my
01:48:29.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m29s

image right and and I kind of take a
01:48:32.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m32s

bigger area few different color I kind
01:48:41.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m41s

of take a bigger area right and I do a
01:48:46.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m46s

convolution using that bigger area in
01:48:49.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m49s

this case I'm doing five by five right
01:48:51.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m51s

then that kind of allows me to try and
01:48:57.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h48m57s

find more interesting richer features in
01:49:00.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m00s

that 5x5 area and so then I spin out a
01:49:04.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m04s

bigger output this case I spit out a
01:49:08.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m08s

filter size good about ten five by five
01:49:11.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m11s

filters and so the idea is like pretty
01:49:14.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m14s

much every state of the art
01:49:17.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m17s

convolutional architecture now starts
01:49:19.429
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m19s

out with a single con flare with like a
01:49:21.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m21s

five by five or seven by seven or
01:49:25.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m25s

sometimes even like 11 by 11 convolution
01:49:27.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m27s

with like quite a few filters you know
01:49:31.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m31s

something like you know thirty two
01:49:36.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m36s

filters coming out and it's just just a
01:49:39.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m39s

way of kind of trying to and like
01:49:42.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m42s

because I use the straight of one and
01:49:44.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m44s

the padding of kernel size minus one
01:49:46.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m46s

over two
01:49:50.119
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m50s

it means that my outputs going to be
01:49:50.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m50s

exactly the same size as my input but
01:49:52.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m52s

just got more filters now this is just a
01:49:55.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m55s

good way of trying to create a richer
01:49:57.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h49m57s

starting point for my sequence of
01:50:00.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m00s

convolutional layers
01:50:03.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m03s

okay so that's the basic theory of why
01:50:04.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m04s

I've added this single convolution which
01:50:08.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m08s

I just do once at the start and then I
01:50:10.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m10s

just go through all my layers and then I
01:50:12.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m12s

do my adaptive max pooling and my final
01:50:14.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m14s

classifier okay so it's a minor tweak
01:50:17.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m17s

but it helps right and so you'll see now
01:50:20.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m20s

I kind of can go for a Moodle I have 60%
01:50:24.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m24s

and after a couple is 45% now after a
01:50:28.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m28s

couple that's 57% and after a few more
01:50:34.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m34s

I'm out for 68% okay
01:50:36.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m36s

so you can see it's you know the the
01:50:38.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m38s

batch norm and you know tiny bit the
01:50:40.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m40s

conveyor at the start it's helping now
01:50:42.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m42s

what's more you can see this is still
01:50:45.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m45s

increasing right so that's looking
01:50:46.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m46s

pretty encouraging okay so given that
01:50:49.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m49s

this is looking pretty good an obvious
01:50:52.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m52s

thing to try might be to see is to try
01:50:54.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h50m54s

increasing the depth of the model and
01:51:01.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m01s

now I can't just add more of most dried
01:51:03.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m03s

to layers because remember how it half
01:51:07.220
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m07s

the size of the image each time I'm
01:51:10.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m10s

basically down to two by two at the end
01:51:13.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m13s

right so I can't add much more so what I
01:51:14.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m14s

did instead was I said okay here's my
01:51:18.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m18s

original layers so you must trade two
01:51:21.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m21s

layers for everyone also create a
01:51:23.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m23s

straight one layer so astrayed one layer
01:51:26.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m26s

doesn't change the size and so now I'm
01:51:29.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m29s

saying zip my stride two layers and my
01:51:31.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m31s

stride one layers together and so first
01:51:36.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m36s

of all do the straight too and then do
01:51:38.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m38s

the straight one so this is now actually
01:51:41.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m41s

twice as deep okay so this is so this is
01:51:43.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m43s

now twice as deep but I end up with the
01:51:50.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m50s

exact same you know two by two that I
01:51:53.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m53s

had before and so if I try this you know
01:51:56.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h51m56s

here after one two three four epochs is
01:52:00.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m00s

at sixty-five percent after one two
01:52:03.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m03s

three epochs I'm still at 65% it hasn't
01:52:06.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m06s

helped right and so the reason it hasn't
01:52:08.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m08s

helped is I'm now too deep
01:52:13.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m13s

even for batch norm two handlers now my
01:52:17.140
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m17s

depth is now one two three four five
01:52:20.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m20s

times two is ten eleven kampf 112 okay
01:52:23.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m23s

so 12 layers deep it's possible to train
01:52:30.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m30s

a standard confident 12 layers deep but
01:52:34.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m34s

it starts to get difficult to do it
01:52:36.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m36s

properly right and it certainly doesn't
01:52:38.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m38s

seem to be really helping much if at all
01:52:40.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m40s

so that's where I'm instead going to
01:52:42.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m42s

replace this with a ResNet all right so
01:52:45.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m45s

a rest net is our final stage
01:52:49.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m49s

what a resin it does is I'm going to
01:52:51.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m51s

replace our BN layer right I'm going to
01:52:55.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m55s

inherit from BN layer and replace our
01:52:58.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h52m58s

forward with that and that's it
01:53:00.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m00s

everything else is going to be identical
01:53:04.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m04s

except now I'm going to do like way lots
01:53:06.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m06s

of layers I'm going to make it four
01:53:09.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m09s

times deeper right and it's going to
01:53:11.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m11s

Train beautifully just because of that
01:53:13.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m13s

so why does that help so much so this is
01:53:17.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m17s

called a ResNet block and as you can see
01:53:22.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m22s

I'm saying that's not what I meant to do
01:53:25.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m25s

I'm saying my predictions equals my
01:53:31.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m31s

input plus some function you know in
01:53:36.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m36s

this case a convolution of my input
01:53:40.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m40s

alright that's that's that's what I've
01:53:42.429
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m42s

written here and so I'm now going to
01:53:44.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m44s

shuffle that around a little bit and I'm
01:53:48.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m48s

going to say I'm going to say f of X
01:53:53.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h53m53s

equals y minus X ok so there's the same
01:54:00.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m00s

thing shuffled around right that's my
01:54:06.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m06s

prediction within the previous layer
01:54:09.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m09s

right and so what this is then doing is
01:54:11.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m11s

it's trying to fit a function to the
01:54:16.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m16s

difference between these two right and
01:54:19.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m19s

so the difference is actually the
01:54:22.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m22s

residual
01:54:28.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m28s

so if this is what I'm trying to
01:54:35.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m35s

calculate my actual Y value and this is
01:54:41.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m41s

the thing that I've most recently
01:54:44.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m44s

calculated then the difference between
01:54:46.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m46s

the two is basically the error in terms
01:54:48.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m48s

of what I've calculated so far and so
01:54:51.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m51s

this is therefore saying that okay try
01:54:54.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m54s

to find a set of convolutional weights
01:54:56.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m56s

that attempts to fill in the the amount
01:54:59.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h54m59s

I was off by so in other words if we
01:55:04.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m04s

let's clear this out if we have some
01:55:08.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m08s

inputs coming in right and then we have
01:55:13.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m13s

this function which is basically trying
01:55:16.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m16s

to predict the error it's like how much
01:55:18.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m18s

are we off by right and then we add that
01:55:21.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m21s

on so we basically add on this
01:55:25.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m25s

additional like prediction of how much
01:55:27.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m27s

will be wrong by and then we add on
01:55:29.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m29s

another prediction of how much were we
01:55:31.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m31s

wrong by that time and add on another
01:55:33.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m33s

prediction of how much we wrong by that
01:55:35.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m35s

time then that each time we're kind of
01:55:37.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m37s

zooming in getting closer and closer to
01:55:40.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m40s

our correct answer and each time we're
01:55:44.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m44s

saying like okay we've got to a certain
01:55:46.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m46s

point but we still got an error you've
01:55:48.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m48s

still got a residual so let's try and
01:55:51.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m51s

create a model that just predicts that
01:55:53.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m53s

residual and add that on to our previous
01:55:56.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m56s

model and then let's build another model
01:55:58.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h55m58s

that predicts the residual and add that
01:56:00.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m00s

on to our previous model and if we keep
01:56:02.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m02s

doing that again and again we should get
01:56:04.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m04s

closer and closer to our answer and this
01:56:06.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m06s

is based on a theory called boosting
01:56:10.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m10s

which people that have done some machine
01:56:14.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m14s

learning will certainly come across
01:56:16.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m16s

right and so basically the trick here is
01:56:17.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m17s

that by specifying that as being the
01:56:20.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m20s

thing that we're trying to calculate
01:56:30.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m30s

then we kind of get boosting for free
01:56:36.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m36s

right it's like because we couldn't just
01:56:39.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m39s

juggle that around to show that actually
01:56:41.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m41s

it's just calculating a model on the
01:56:44.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m44s

wrist Jill so that's kind of amazing and
01:56:47.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m47s

you know it totally works as you can see
01:56:53.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m53s

here I've now got my standard batch norm
01:56:56.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m56s

layer okay which is something which is
01:56:59.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h56m59s

going to reduce my size by two because
01:57:02.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m02s

it's got the stride too and then I've
01:57:05.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m05s

got a resident layers dried one and
01:57:07.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m07s

another resident layer astride one right
01:57:08.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m08s

and sorry I think I said that was four
01:57:11.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m11s

of these is actually three of these so
01:57:13.739
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m13s

this is now three times deeper I zipped
01:57:15.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m15s

through all of those and so I've now got
01:57:17.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m17s

a function of a function of a function
01:57:19.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m19s

so three layers per group and then my
01:57:22.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m22s

con at the start and my linear at the
01:57:26.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m26s

end so this is now three times bigger
01:57:29.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m29s

than my original and if I fit it you can
01:57:32.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m32s

see it's just keeps going up and up and
01:57:37.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m37s

up and up I keep fitting it more he's
01:57:39.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m39s

going up and up and it's still going up
01:57:41.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m41s

when I kind of got bored okay so the
01:57:45.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m45s

rest net has been a really important
01:57:49.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m49s

development and it's allowed us to
01:57:54.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m54s

create these really deep networks right
01:57:58.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h57m58s

now the full risk net does not quite
01:58:03.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m03s

look the way I've described it here the
01:58:06.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m06s

full res net doesn't just have one
01:58:09.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m09s

convolution right but it actually has
01:58:11.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m11s

two convolutions right so the way people
01:58:14.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m14s

normally draw resident blocks is they
01:58:16.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m16s

normally say you've got some input
01:58:19.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m19s

coming in to the layer it goes through
01:58:21.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m21s

one convolution to convolutions and then
01:58:26.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m26s

gets added back to the original input
01:58:31.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m31s

right that's the full version of a
01:58:35.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m35s

ResNet block in my case I've just done
01:58:37.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m37s

one convolution okay and then you'll see
01:58:40.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m40s

also in every block
01:58:43.409
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m43s

right one of them it's actually the
01:58:47.959
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m47s

first one does he it's actually the
01:58:52.729
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m52s

first one here is not a resident block
01:58:56.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m56s

but a standard convolution with a stride
01:58:59.209
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h58m59s

of two right this is called a bottleneck
01:59:04.249
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m04s

layer right and the idea is this is not
01:59:07.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m07s

a ResNet block so from time to time we
01:59:09.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m09s

actually change the geometry right we're
01:59:12.769
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m12s

doing this trade to in resident we don't
01:59:14.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m14s

actually use just a standard
01:59:17.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m17s

convolutional layer there's actually a
01:59:19.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m19s

different form of bottleneck block that
01:59:21.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m21s

I'm not going to picture in this course
01:59:23.959
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m23s

I'm going to teach you in part two okay
01:59:25.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m25s

but as you can see even this somewhat
01:59:27.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m27s

simplified version of a resin it still
01:59:29.749
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m29s

works pretty well and so we can make it
01:59:31.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m31s

a little bit bigger all right and so
01:59:35.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m35s

here I've just increased all of my sizes
01:59:38.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m38s

I have still got my three and also I've
01:59:41.229
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m41s

had it drop out right so at this point
01:59:45.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m45s

I'm gonna say this is other than the
01:59:47.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m47s

minor simplification of ResNet you know
01:59:49.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m49s

a reasonable approximation of a good
01:59:52.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m52s

starting point for a modern architecture
01:59:54.979
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m54s

okay and so now I've added in my point
01:59:56.979
https://www.youtube.com/watch?v=H3g26EVADgY#t=01h59m56s

to drop out I've increased the size here
02:00:00.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m00s

and if I train this you know I can treat
02:00:02.479
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m02s

it for a while it's going pretty well I
02:00:06.289
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m06s

can then add in gta at the end
02:00:08.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m08s

eventually I get 85% and you know this
02:00:09.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m09s

is at a point now where like literally I
02:00:13.189
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m13s

wrote this whole notebook in like three
02:00:16.249
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m16s

hours right we can like create this
02:00:17.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m17s

thing in three hours and this is like an
02:00:19.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m19s

accuracy that in kind of 2012-2013 was
02:00:22.519
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m22s

considered pretty much data the art for
02:00:26.389
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m26s

say pocket right so this is actually no
02:00:29.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m29s

this is actually pretty damn good to get
02:00:32.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m32s

you know nowadays the most recent
02:00:35.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m35s

results are like 97% you know there are
02:00:37.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m37s

there's plenty of room we can still
02:00:40.789
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m40s

improve but they're all based on these
02:00:42.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m42s

techniques like there isn't really
02:00:44.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m44s

anything you know when we start looking
02:00:46.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m46s

in in part to it like how to get this
02:00:49.999
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m49s

right up to state of the art you'll see
02:00:52.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m52s

it's basically better approaches to data
02:00:53.959
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m53s

augmentation better approaches to
02:00:55.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m55s

regularization some tweaks on ResNet
02:00:57.979
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h00m57s

but it's all basically the circuit okay
02:01:01.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m01s

so so is the residual training on the
02:01:04.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m04s

residual method is that only looks like
02:01:09.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m09s

it's a generic thing that can be applied
02:01:13.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m13s

non image problems oh great question
02:01:16.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m16s

yeah yes it is but it's like being
02:01:19.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m19s

ignored everywhere else in NLP something
02:01:22.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m22s

called the transformer architecture
02:01:25.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m25s

recently appeared and you know was shown
02:01:28.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m28s

to be the state of the art for
02:01:31.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m31s

translation and it's got like a simple
02:01:32.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m32s

resonance structure you know first time
02:01:36.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m36s

I've ever seen it in NLP I haven't
02:01:38.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m38s

really seen anybody else take advantage
02:01:40.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m40s

of it yeah this general approach we call
02:01:42.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m42s

these skip connections this idea of like
02:01:45.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m45s

skipping over a layer and kind of doing
02:01:47.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m47s

an identity it's yeah it's been
02:01:49.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m49s

appearing a lot in computer vision and
02:01:52.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m52s

nobody else much seems to be using it
02:01:54.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m54s

even though there's nothing computer
02:01:56.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m56s

vision specific about it so I think it's
02:01:57.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h01m57s

a big opportunity okay so final stage I
02:02:00.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m00s

want to show you is how to use an extra
02:02:05.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m05s

feature of Pi torch to do something cool
02:02:09.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m09s

and it's going to be a kind of a segue
02:02:12.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m12s

into part two it's going to be our first
02:02:14.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m14s

little hint as to what else we can build
02:02:17.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m17s

on these neural nets and so and it's
02:02:20.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m20s

also going to take us all the way back
02:02:23.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m23s

to lesson 1 which is we're going to do
02:02:24.490
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m24s

dogs and cats ok so going all the way
02:02:26.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m26s

back to dogs and cats we're going to
02:02:29.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m29s

create a resin at 34 ok so these
02:02:32.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m32s

different ResNet 3450 101 they're
02:02:34.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m34s

they're basically just different numbers
02:02:38.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m38s

of different sized blocks it's like how
02:02:42.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m42s

many of these kind of pieces do you have
02:02:46.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m46s

before it bottleneck block and then how
02:02:48.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m48s

many of these sets of super blocks do
02:02:50.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m50s

you have right that's all these
02:02:53.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m53s

different numbers mean so if you look at
02:02:54.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m54s

the torch vision source code you can
02:02:56.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h02m56s

actually see the definition of these
02:03:00.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m00s

different resonates you'll see they're
02:03:01.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m01s

all just different parameters right ok
02:03:03.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m03s

so we're going to use rest at 34
02:03:09.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m09s

and so we're going to do this a little
02:03:11.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m11s

bit more by hand okay so if this is my
02:03:13.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m13s

architecture this is just the name of a
02:03:17.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m17s

function then I can call it to get that
02:03:19.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m19s

model right and then true look at the
02:03:22.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m22s

definition is do I want the pre-trained
02:03:25.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m25s

so in other words is it going to load in
02:03:27.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m27s

the pre-trained imagenet weights
02:03:29.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m29s

okay so M now contains a model and so I
02:03:31.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m31s

can take a look at it like so okay and
02:03:36.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m36s

so you can see here what's going on
02:03:39.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m39s

right is that inside here I've got my
02:03:41.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m41s

initial two deconvolution
02:03:47.510
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m47s

and here is that kernel size of seven by
02:03:49.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m49s

seven okay and interestingly in this
02:03:52.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m52s

case it actually starts out with a seven
02:03:54.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m54s

by seven strobe - okay there's the
02:03:56.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m56s

padding that we talked about to make
02:03:58.890
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h03m58s

sure that we don't lose the edges all
02:04:00.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m00s

right there's our batch naught okay
02:04:02.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m02s

there's our Lu you get the idea right
02:04:04.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m04s

Kong and then so here you can now see
02:04:07.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m07s

there's a layer that contains a bunch of
02:04:10.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m10s

blocks all right so here's a block which
02:04:13.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m13s

contains a cons fetch norm rally you con
02:04:16.170
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m16s

Bethnal you can't see it printed but
02:04:19.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m19s

after this is where it does the addition
02:04:22.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m22s

all right so there's like a whole ResNet
02:04:24.660
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m24s

block and then another resident block
02:04:26.820
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m26s

and then another ResNet block okay and
02:04:28.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m28s

then you can see also sometimes you see
02:04:33.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m33s

one where there's a stripe - right so
02:04:38.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m38s

here's actually one of these bottleneck
02:04:40.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m40s

layers okay
02:04:44.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m44s

so you can kind of see how this is this
02:04:47.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m47s

is structure so in our case sorry I
02:04:49.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m49s

skipped over this a little bit but the
02:04:53.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m53s

approach that we ended up using for real
02:04:59.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h04m59s

you was to put it before our before our
02:05:04.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m04s

Bashan on which see what they do here
02:05:12.719
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m12s

we've got fetch norm railing you cons
02:05:18.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m18s

that
02:05:24.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m24s

no I'm really okay okay so you can see
02:05:25.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m25s

the order that they're using it here
02:05:26.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m26s

okay
02:05:27.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m27s

and you'll find like there's two
02:05:29.309
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m29s

different versions of ResNet in fact
02:05:31.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m31s

there's three different versions of
02:05:33.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m33s

ResNet floating around the one which
02:05:34.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m34s

actually turns out to be the best it's
02:05:37.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m37s

called the pre-act ResNet which has a
02:05:39.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m39s

different ordering again but you can
02:05:41.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m41s

look it up it's basically a different
02:05:47.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m47s

order of where the value and where the
02:05:48.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m48s

batch norm yeah okay so we're going to
02:05:50.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m50s

start with a standard resident 34 and
02:05:53.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m53s

normally what we do is we need to now
02:05:57.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h05m57s

turn this into something that can
02:06:01.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m01s

predict dogs versus class right so
02:06:02.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m02s

currently the final layer has a thousand
02:06:06.329
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m06s

features because imagenet has a thousand
02:06:10.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m10s

features right so we need to get rid of
02:06:12.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m12s

this so when you use confer owner from
02:06:14.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m14s

pre-trained in fast AI it actually
02:06:19.679
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m19s

deletes this layer for you and it also
02:06:22.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m22s

deletes this layer and something that as
02:06:25.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m25s

far as I know is unique to fast AI is we
02:06:30.150
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m30s

replace this see this is an average
02:06:33.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m33s

pooling layer of size seven by seven
02:06:35.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m35s

right so this is the basically the
02:06:37.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m37s

adaptive pooling layer but whoever wrote
02:06:39.869
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m39s

this didn't know about adaptive pooling
02:06:41.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m41s

so they manually said oh I know it's
02:06:43.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m43s

meant to be seven by seven so in first
02:06:45.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m45s

AI we replaced this with adaptive
02:06:47.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m47s

pooling but we actually do both adaptive
02:06:49.619
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m49s

average pooling and adaptive max pooling
02:06:51.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m51s

and we then concatenate them two
02:06:54.869
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m54s

together which it's it is something we
02:06:56.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h06m56s

invented but at the same time we
02:07:00.239
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m00s

invented it somebody wrote a paper about
02:07:02.040
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m02s

it
02:07:03.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m03s

so it's you know we don't get any credit
02:07:04.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m04s

but I think we're the only library that
02:07:06.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m06s

provides it and certainly only one that
02:07:08.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m08s

does it by default we're going to for
02:07:09.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m09s

the purpose of this exercise though
02:07:14.130
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m14s

we're going to do a simple version where
02:07:15.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m15s

we delete the last two layers so we'll
02:07:17.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m17s

grab all the children of the model will
02:07:19.619
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m19s

delete the last two layers and then
02:07:21.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m21s

instead we're going to add a convolution
02:07:24.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m24s

which just has two outputs right I'll
02:07:27.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m27s

show you why in a moment
02:07:32.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m32s

right then we're going to do our average
02:07:33.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m33s

pooling and then we're going to do
02:07:36.599
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m36s

soft mess okay so that's a model which
02:07:39.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m39s

is going to have you'll see that there
02:07:43.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m43s

is no this one has a fully connected
02:07:46.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m46s

layer at the end this one does not have
02:07:48.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m48s

a fully connected layer yet but if you
02:07:50.640
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m50s

think about it this convolutional layer
02:07:53.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m53s

is going to be two filters only right
02:07:55.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h07m55s

and it's going to be two by seven by
02:08:01.380
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m01s

seven and so once we then do the average
02:08:03.900
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m03s

pooling it's going to end up being just
02:08:06.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m06s

two numbers that it produces so this is
02:08:09.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m09s

a different way of producing just two
02:08:11.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m11s

numbers I'm not going to say it's better
02:08:13.230
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m13s

it's going to say it's different okay
02:08:15.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m15s

but there's a reason we do it I'll show
02:08:16.920
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m16s

you the reason we can now train this
02:08:19.470
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m19s

model in the usual way right so we can
02:08:21.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m21s

say transform stock model image
02:08:23.910
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m23s

classifier data from paths and then we
02:08:26.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m26s

can use that Kampf learner from model
02:08:29.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m29s

data we just learnt about I'm now going
02:08:30.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m30s

to freeze every single layer except for
02:08:34.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m34s

that one and this is the fourth last
02:08:38.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m38s

layer
02:08:41.400
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m41s

so we'll say freeze to minus four right
02:08:41.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m41s

and so this is just training the last
02:08:44.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m44s

layer okay so we get 99.1 percent
02:08:46.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m46s

accuracy so that you know this
02:08:49.350
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m49s

approaches working fine and here's what
02:08:50.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m50s

we can do though we can now do something
02:08:53.700
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m53s

called fast comm last activated Maps
02:08:56.160
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h08m56s

fast activation is what we're going to
02:09:02.300
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m02s

do is we're going to try to look at this
02:09:06.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m06s

particular cat and we're going to use a
02:09:08.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m08s

technique called class activation Maps
02:09:11.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m11s

where we take our model and we ask you
02:09:13.620
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m13s

which parts of this image turned out to
02:09:16.320
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m16s

be important and when we do this it's
02:09:19.290
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m19s

going to feed out this is got the
02:09:22.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m22s

picture it's going to create alright and
02:09:24.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m24s

so as you can see here it's found the
02:09:26.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m26s

cat so how did it do that well the way
02:09:28.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m28s

it did that will kind of work backwards
02:09:32.580
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m32s

is to produce this matrix now you'll see
02:09:33.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m33s

in this matrix there's some pretty big
02:09:38.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m38s

numbers around about here which
02:09:41.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m41s

correspond to our cat so what is this
02:09:44.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m44s

matrix this matrix is simply
02:09:48.450
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m48s

or to the value of this feature matrix
02:09:52.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h09m52s

times this py vector the py vector is
02:10:00.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m00s

simply equal to the predictions which in
02:10:05.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m05s

this case said I am 100% confident it's
02:10:09.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m09s

a cat right so this is just equal to the
02:10:11.970
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m11s

value of if I just call the model
02:10:14.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m14s

passing in our cat this is our cat
02:10:17.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m17s

that's an X then we got our predictions
02:10:20.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m20s

right so it's just the value of our
02:10:23.430
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m23s

predictions so py is just the value of
02:10:24.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m24s

our predictions
02:10:27.180
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m27s

what about feet what's that equal to
02:10:27.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m27s

feet is equal to the values in this
02:10:30.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m30s

layer right in other words the value
02:10:38.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m38s

that comes out of the final in facts
02:10:40.740
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m40s

come out of this ladder coming out of
02:10:42.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m42s

the final convolutional layer right so
02:10:43.950
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m43s

it's actually the seven by seven by two
02:10:46.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m46s

and so you can see here let's see feet
02:10:50.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m50s

the shape of features is two filters by
02:10:54.570
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m54s

seven by seven right so the idea is if
02:10:58.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h10m58s

we multiply that vector by that tensor
02:11:03.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m03s

right then it's going to end up grabbing
02:11:09.650
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m09s

all of the first channel because that's
02:11:12.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m12s

a 1 and none of the second channel
02:11:15.240
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m15s

because that's a 0 and so therefore it's
02:11:17.790
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m17s

going to return the value of the last
02:11:21.390
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m21s

convolutional layer for the for the
02:11:23.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m23s

section which lines up with being a cat
02:11:28.080
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m28s

but if you think about it this the first
02:11:30.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m30s

section lines up with being a cat the
02:11:33.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m33s

second section lines up with being a dog
02:11:36.120
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m36s

so if we multiply that tensor by that
02:11:37.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m37s

tensor we end up with this matrix and
02:11:41.100
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m41s

this matrix is which parts most like a
02:11:45.330
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m45s

cat or to put it another way in our
02:11:50.010
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m50s

model the only thing that happened after
02:11:53.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m53s

the convolutional layer was an average
02:11:56.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m56s

pooling layer so the average pooling
02:11:59.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h11m59s

layer talked that 7x7 grid and said
02:12:01.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m01s

average
02:12:04.410
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m04s

how much each part is cat Lake that
02:12:05.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m05s

answered my final value my final
02:12:08.929
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m08s

prediction was the average cattiness
02:12:10.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m10s

there's the whole thing right and so
02:12:13.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m13s

because it had to be able to average out
02:12:17.830
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m17s

these things to get the average
02:12:19.969
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m19s

cattiness that means I could then just
02:12:20.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m20s

take this matrix and resize it to be the
02:12:23.449
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m23s

same size as my original cat and just
02:12:28.310
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m28s

overlay it on top you get this heat map
02:12:30.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m30s

right so the way you can use this
02:12:33.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m33s

technique at home is to basically
02:12:35.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m35s

calculate this matrix right on some like
02:12:39.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m39s

really you've got some really big
02:12:41.870
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m41s

picture you can calculate this matrix on
02:12:43.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m43s

a quick small little cognate and then
02:12:46.489
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m46s

zoom into the bit that has the highest
02:12:48.860
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m48s

value and then rerun it just on that
02:12:51.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m51s

part that so it's like oh this is the
02:12:54.560
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m54s

area that seems to be the most like a
02:12:57.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m57s

hat or most like a dog that zoom in to
02:12:58.550
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h12m58s

that bit right so I skipped over that
02:13:01.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m01s

pretty quickly because we ran out of
02:13:05.420
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m05s

time and so we'll be learning more about
02:13:06.530
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m06s

these kind of approaches in part two and
02:13:10.940
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m10s

we can talk about it more on the forum
02:13:12.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m12s

and hopefully you get the idea the one
02:13:13.969
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m13s

thing that totally skipped over was how
02:13:15.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m15s

do we actually ask for that particular
02:13:18.199
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m18s

layer okay and I'll let you read about
02:13:21.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m21s

this during the week but basically
02:13:23.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m23s

there's a thing called a hook so we said
02:13:25.670
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m25s

we called save features which is this
02:13:31.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m31s

little class that we wrote that goes
02:13:35.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m35s

register forward hook and basically a
02:13:37.370
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m37s

forward hook is a special PI torch thing
02:13:40.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m40s

that every time it calculates a layer it
02:13:42.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m42s

runs this function it's like a callback
02:13:45.949
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m45s

basically it's like a callback that
02:13:48.880
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m48s

happens every time it calculates a layer
02:13:50.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m50s

and so in this case it just saved the
02:13:52.250
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m52s

value of the particular layer that I was
02:13:55.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m55s

interested in okay and so that way I was
02:13:59.060
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h13m59s

able to go inside here and grab those
02:14:02.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m02s

features out look after I was done okay
02:14:04.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m04s

so I call save features that gives me my
02:14:10.909
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m10s

pork and then later on I can just grab
02:14:13.730
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m13s

the value that I saved okay so I skipped
02:14:15.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m15s

over that pretty
02:14:18.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m18s

quickly but if you look in the pipe or
02:14:19.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m19s

docks they have some more information
02:14:20.840
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m20s

and help about that yes Jeremy can you
02:14:22.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m22s

spend five minutes talking about your
02:14:28.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m28s

journey into deep learning
02:14:31.699
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m31s

yeah and finally how can we keep up with
02:14:34.719
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m34s

important research that is important to
02:14:39.340
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m39s

practice sure yeah so it's gonna that's
02:14:42.050
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m42s

good I think I'll close more on the
02:14:46.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m46s

latter bit which is like what now okay
02:14:47.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m47s

so for those of you are interested you
02:14:50.090
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m50s

should aim to come back for part two if
02:14:55.099
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m55s

you're aiming to come back for part two
02:14:57.260
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m57s

how many people would like to come back
02:14:59.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h14m59s

to part two okay that's not bad I think
02:15:00.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m00s

almost everybody so if you want to come
02:15:02.989
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m02s

back to part two be aware of this by
02:15:05.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m05s

that time you're expected to have
02:15:08.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m08s

mastered all of the techniques we've
02:15:10.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m10s

learnt in part one and there's plenty of
02:15:12.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m12s

time between now and then okay even if
02:15:14.210
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m14s

you haven't done much or any ml before
02:15:16.280
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m16s

but it does assume that you're going to
02:15:18.800
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m18s

be working you know at the same level of
02:15:20.960
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m20s

intensity for now until then that you
02:15:23.750
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m23s

have been with practicing right so
02:15:25.760
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m25s

practicing so generally speaking the
02:15:27.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m27s

people who did well in part two last
02:15:30.469
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m30s

year had watched each of the videos
02:15:32.179
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m32s

about three times right and some of the
02:15:34.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m34s

people actually I knew had actually
02:15:36.980
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m36s

discovered they learnt some of them off
02:15:39.500
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m39s

by heart by mistake so they're like
02:15:40.610
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m40s

watching the video is again is helpful
02:15:42.590
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m42s

and make sure you get to the point that
02:15:44.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m44s

you can recreate the notebooks without
02:15:45.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m45s

watching the videos all right and so
02:15:48.349
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m48s

other men make more interesting
02:15:50.810
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m50s

obviously try and recreate them
02:15:51.710
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m51s

notebooks using different data sets you
02:15:53.630
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m53s

know and definitely then just keep up
02:15:57.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h15m57s

with the forum and you'll see people
02:16:00.020
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m00s

keep on posting more stuff about recent
02:16:01.460
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m01s

papers and recent advances and over the
02:16:04.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m04s

next couple of months you'll find
02:16:07.070
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m07s

increasingly less and less of it seems
02:16:08.540
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m08s

weird mysterious and more and more of it
02:16:10.659
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m10s

makes perfect sense and so it's a bit of
02:16:13.219
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m13s

a case with just thing staying tenacious
02:16:17.030
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m17s

you know there's always going to be
02:16:19.369
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m19s

stuff that you don't understand yet and
02:16:20.719
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m20s

but you'll be surprised if you go back
02:16:23.000
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m23s

to lesson one and two now you'll be like
02:16:25.849
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m25s

oh that's all trivial right
02:16:28.159
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m28s

so you know that's kind of hopefully a
02:16:32.381
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m32s

bit of your learning journey and yeah I
02:16:34.360
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m34s

think the main thing I've noticed is
02:16:38.680
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m38s

that people who succeed are the ones who
02:16:40.780
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m40s

just keep keep working at it you know so
02:16:42.251
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m42s

not coming back here every Monday you're
02:16:44.770
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m44s

not going to have that forcing function
02:16:46.720
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m46s

I've noticed the forum suddenly gets
02:16:48.520
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m48s

busy at 5:00 p.m. on a Monday you know
02:16:51.461
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m51s

it's like Oh course is about to start
02:16:53.711
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m53s

and suddenly these questions start
02:16:55.690
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m55s

coming in so now that you don't have
02:16:57.041
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m57s

that forcing function you know try and
02:16:58.600
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h16m58s

use some other technique to you know
02:17:00.850
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m00s

give yourself that little kick maybe you
02:17:03.581
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m03s

can tell your partner at home
02:17:05.110
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m05s

you know I'm going to try and produce
02:17:06.671
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m06s

something every Saturday for the next
02:17:08.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m08s

four weeks or I'm going to try and
02:17:10.001
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m10s

finish reading this paper or something
02:17:11.440
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m11s

you know anyway so I hope to see you all
02:17:13.990
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m13s

back in March and even I regardless
02:17:17.381
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m17s

whether I do or don't it's been a really
02:17:21.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m21s

great pleasure to get to know you all
02:17:22.480
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m22s

and I hope to keep seeing on the forum
02:17:24.190
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m24s

thanks very much
02:17:26.501
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m26s

[Applause]
02:17:29.270
https://www.youtube.com/watch?v=H3g26EVADgY#t=02h17m29s

